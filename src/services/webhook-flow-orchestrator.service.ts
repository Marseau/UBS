/**
 * Webhook Flow Orchestrator Service
 * Orquestra integra√ß√£o do Flow Lock System com webhook existente
 * Baseado na OS: Sincronizar Inten√ß√µes e Evitar Mistura
 */

import { DeterministicIntentDetectorService, detectIntents, INTENT_KEYS, detectIntentByRegex } from './deterministic-intent-detector.service';
import { FlowLockManagerService } from './flow-lock-manager.service';
import { ConversationOutcomeAnalyzerService } from './conversation-outcome-analyzer.service';

const SYSTEM_STANDARD_RESPONSES: string[] = [
  'S√≥ para confirmar: voc√™ quer *servi√ßos*, *pre√ßos* ou *hor√°rios*?',
  'Infelizmente neste momento n√£o possuo esta informa√ß√£o no sistema.'
];
import { mergeEnhancedConversationContext } from '../utils/conversation-context-helper';
import { EnhancedConversationContext, FlowType, FlowStep } from '../types/flow-lock.types';
import OpenAI from 'openai';
import { MODELS, getModelForContext } from '../utils/ai-models';

export interface WebhookOrchestrationResult {
  aiResponse: string;
  shouldSendWhatsApp: boolean;
  conversationOutcome: string | null; // null se conversa em andamento
  updatedContext: EnhancedConversationContext;
  telemetryData: {
    intent: string | null;
    confidence: number;
    decision_method: string;
    flow_lock_active: boolean;
    processing_time_ms: number;
    model_used?: string; // üöÄ Modelo usado nas m√©tricas LLM
  };
  llmMetrics?: {
    prompt_tokens: number | null;
    completion_tokens: number | null;
    total_tokens: number | null;
    api_cost_usd: number | null;
    processing_cost_usd: number | null;
    confidence_score: number | null;
    latency_ms: number | null;
  };
}

// Resolve op√ß√µes simples de desambigua√ß√£o (pt-BR)
function resolveDisambiguationChoice(text: string): string | null {
  const t = (text || '').normalize('NFD').replace(/[\u0300-\u036f]/g, '').toLowerCase();
  if (/(servicos?|lista|catalogo)/i.test(t)) return 'services';
  if (/(precos?|preco|valores?|quanto|orcamento)/i.test(t)) return 'pricing';
  if (/(horarios?|agenda|disponivel|amanha|hoje|quando)/i.test(t)) return 'availability';
  return null;
}

export class WebhookFlowOrchestratorService {
  private intentDetector: DeterministicIntentDetectorService;
  private flowManager: FlowLockManagerService;
  private outcomeAnalyzer: ConversationOutcomeAnalyzerService;
  private openai: OpenAI;

  constructor() {
    this.intentDetector = new DeterministicIntentDetectorService();
    this.flowManager = new FlowLockManagerService();
    this.outcomeAnalyzer = new ConversationOutcomeAnalyzerService();
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY || ''
    });
  }

  /**
   * Processamento principal do webhook com Flow Lock
   */
  async orchestrateWebhookFlow(
    messageText: string,
    userId: string,
    tenantId: string,
    tenantConfig: any,
    existingContext?: any
  ): Promise<WebhookOrchestrationResult> {
    const startTime = Date.now();

    try {
      // 1. Resolver contexto enhanced (com flow_lock)
      const context = await this.resolveEnhancedContext(
        userId, 
        tenantId, 
        tenantConfig,
        existingContext
      );

      // 2. Verificar timeout de fluxo ativo
      const timeoutStatus = this.flowManager.checkTimeoutStatus(context);
      if (timeoutStatus.status === 'expired') {
        return await this.handleExpiredFlow(context, timeoutStatus.message || '');
      }
      if (timeoutStatus.status === 'warning') {
        return this.handleFlowWarning(context, timeoutStatus.message || '');
      }

      // 2.5. Se estamos aguardando escolha de inten√ß√£o (desambigua√ß√£o), resolva primeiro
      if (context && (context as any).awaiting_intent === true) {
        const choice = resolveDisambiguationChoice(messageText);
        if (choice) {
          // limpamos a flag no contexto e seguimos com a intent resolvida
          const updatedCtx = await mergeEnhancedConversationContext(
            userId,
            tenantId,
            context,
            { intent: choice, decision_method: 'llm', confidence: 1.0 }
          );
          return await this.orchestrateWebhookFlow(messageText, userId, tenantId, tenantConfig, updatedCtx);
        } else {
          // ainda amb√≠guo ‚Üí pergunta novamente, sem efeitos colaterais
          return {
            aiResponse: 'S√≥ para confirmar: voc√™ quer *servi√ßos*, *pre√ßos* ou *hor√°rios*?',
            shouldSendWhatsApp: true,
            conversationOutcome: null,
            updatedContext: context,
            telemetryData: {
              intent: null,
              confidence: 0,
              decision_method: 'disambiguation_pending',
              flow_lock_active: !!context.flow_lock?.active_flow,
              processing_time_ms: 0,
              model_used: 'disambiguation' // üöÄ Modelo de desambigua√ß√£o para telemetry
            }
          };
        }
      }

      // 3. Detec√ß√£o de inten√ß√£o - Camadas Regex ‚Üí LLM
      const first = detectIntentByRegex(messageText);
      let finalIntent = first.intent;
      let finalConfidence = first.confidence;
      let decision_method: 'regex' | 'llm' = first.decision_method;

      if (!finalIntent) {
        try {
          const llm = await this.classifyIntentWithLLMFallback(messageText);
          // Se LLM retornar 'null' (unknown), mantemos null (N√ÉO for√ßar 'general')
          finalIntent = (llm?.intent as any) ?? null;
          finalConfidence = (typeof llm?.confidence === 'number') ? llm.confidence : 0.0;
          decision_method = 'llm';
        } catch {
          finalIntent = null;
          finalConfidence = 0.0;
          decision_method = 'llm';
        }
      }

      // se ainda null ‚Üí desambigua√ß√£o (camada 3)
      if (!finalIntent) {
        const updatedCtx = await mergeEnhancedConversationContext(
          userId,
          tenantId,
          context,
          { intent: 'unknown', confidence: 0, decision_method: 'llm' }
        );

        return {
          aiResponse: 'S√≥ para confirmar: voc√™ quer *servi√ßos*, *pre√ßos* ou *hor√°rios*?',
          shouldSendWhatsApp: true,
          conversationOutcome: null,
          updatedContext: updatedCtx,
          telemetryData: {
            intent: null,
            confidence: 0,
            decision_method: 'disambiguation',
            flow_lock_active: !!updatedCtx.flow_lock?.active_flow,
            processing_time_ms: Date.now() - startTime,
            model_used: 'disambiguation' // üöÄ Modelo de desambigua√ß√£o para telemetry
          }
        };
      }
      
      // ‚úÖ ADAPTER: Converter para formato esperado pelo resto do c√≥digo
      const intentResult = {
        intent: finalIntent,
        confidence: finalConfidence,
        decision_method: decision_method,
        allowed_by_flow_lock: true
      } as const;

      // üö® CORRE√á√ÉO: SEMPRE usar OpenAI para gerar respostas reais e capturar m√©tricas LLM
      // Removido curto-circuito determin√≠stico que impedia uso do OpenAI

      // 4. Verificar se inten√ß√£o √© permitida pelo flow lock
      if (!intentResult.allowed_by_flow_lock) {
        return this.handleBlockedIntent(context, intentResult);
      }

      // 5. Determinar novo fluxo baseado na inten√ß√£o
      const targetFlow = this.mapIntentToFlow(intentResult.intent);
      const flowDecision = this.flowManager.canStartNewFlow(context, targetFlow);

      // 6. üö® CORRE√á√ÉO CR√çTICA: Sempre usar OpenAI para gerar resposta real
      // Flow Lock apenas gerencia estado, OpenAI gera TODAS as respostas
      console.log('üîç ORQUESTRADOR DEBUG - Antes de chamar generateAIResponseWithFlowContext:', {
        intent: intentResult.intent,
        flowLock: context.flow_lock?.active_flow,
        messageText: messageText.substring(0, 50)
      });
      
      const result = await this.generateAIResponseWithFlowContext(
        messageText,
        intentResult,
        flowDecision,
        context,
        tenantConfig
      );
      
      console.log('üîç ORQUESTRADOR DEBUG - Resultado generateAIResponseWithFlowContext:', {
        hasLLMMetrics: !!result.llmMetrics,
        outcome: result.outcome,
        responseLength: result.response.length
      });

      // 7. Atualizar contexto com novo estado
      const updatedContext = await this.updateContextWithFlowState(
        userId,
        tenantId,
        context,
        result.newFlowLock,
        intentResult
      );

      // üö® CORRE√á√ÉO: S√≥ persistir outcome se conversa finalizada
      const finalOutcome = this.shouldPersistOutcome(intentResult.intent, result.response, updatedContext);
      console.log('üîß OUTCOME PERSISTENCE CHECK:', {
        intent: intentResult.intent,
        shouldPersist: finalOutcome !== null,
        outcome: finalOutcome
      });

      return {
        aiResponse: result.response,
        shouldSendWhatsApp: true,
        conversationOutcome: finalOutcome, // null se conversa em andamento
        llmMetrics: result.llmMetrics, // üö® CORRE√á√ÉO: Incluir m√©tricas LLM no retorno
        updatedContext,
        telemetryData: {
          intent: finalIntent,              // pode ser null ‚Äî e assim deve ficar
          confidence: finalConfidence,
          decision_method,
          flow_lock_active: !!updatedContext.flow_lock?.active_flow,
          processing_time_ms: Date.now() - startTime,
          model_used: result.llmMetrics?.model_used || 'unknown' // üöÄ Incluir modelo usado no telemetry
        }
      };

    } catch (error) {
      console.error('Webhook orchestration error:', error);
      
      // Fallback para sistema legado
      return {
        aiResponse: 'Desculpe, ocorreu um erro. Tente novamente.',
        shouldSendWhatsApp: true,
        conversationOutcome: 'error',
        updatedContext: await this.resolveEnhancedContext(userId, tenantId, tenantConfig, existingContext),
        telemetryData: {
          intent: 'error',
          confidence: 0,
          decision_method: 'error',
          flow_lock_active: false,
          processing_time_ms: Date.now() - startTime,
          model_used: 'error' // üöÄ Modelo de erro para telemetry
        },
        llmMetrics: {
          prompt_tokens: null,
          completion_tokens: null,
          total_tokens: null,
          api_cost_usd: null,
          processing_cost_usd: null,
          confidence_score: null,
          latency_ms: null
        }
      };
    }
  }

  /**
   * Resolve contexto enhanced (backward compatible)
   */
  private async resolveEnhancedContext(
    userId: string,
    tenantId: string,
    tenantConfig: any,
    existingContext?: any
  ): Promise<EnhancedConversationContext> {
    
    const baseUpdates = {
      tenant_id: tenantId,
      domain: tenantConfig.domain || 'general',
      source: 'whatsapp' as const,
      mode: 'prod' as const
    };

    // Se j√° temos contexto legado, converter para enhanced
    if (existingContext) {
      return await mergeEnhancedConversationContext(
        userId,
        tenantId,
        { ...existingContext, ...baseUpdates }
      );
    }

    // Criar novo contexto enhanced
    return await mergeEnhancedConversationContext(
      userId,
      tenantId,
      baseUpdates
    );
  }

  /**
   * Mapeia intent para flow type
   */
  private mapIntentToFlow(intent: string | null): FlowType {
    if (!intent) return null;
    const flowMap: Record<string, FlowType> = {
      'onboarding': 'onboarding',
      'booking': 'booking',
      'booking_confirm': 'booking',
      'slot_selection': 'booking',
      'reschedule': 'reschedule',
      'reschedule_confirm': 'reschedule', 
      'cancel': 'cancel',
      'cancel_confirm': 'cancel',
      'pricing': 'pricing',
      'services': 'pricing',
      'flow_cancel': null,
      'greeting': null
    };

    // Intents institucionais n√£o criam fluxo
    if (intent.startsWith('institutional_')) return null;

    return flowMap[intent] || null;
  }

  /**
   * Executa a√ß√£o do fluxo baseada na decis√£o
   */
  private async executeFlowAction(
    messageText: string,
    intentResult: any,
    flowDecision: any,
    context: EnhancedConversationContext,
    tenantConfig: any
  ): Promise<{ response: string; outcome: string; newFlowLock?: any }> {

    const intent = intentResult.intent;
    const currentFlow = context.flow_lock?.active_flow;
    const currentStep = context.flow_lock?.step;

    // === COMANDO DIRETOS (prioridade m√°xima) ===
    
    if (intent === 'flow_cancel') {
      const abandonedLock = this.flowManager.abandonFlow(context, 'user_requested');
      return {
        response: 'Cancelado. Como posso ajudar?',
        outcome: 'flow_cancelled',
        newFlowLock: abandonedLock
      };
    }

    if (intent === 'booking_confirm' && currentFlow === 'booking') {
      const completedLock = this.flowManager.completeFlow(context, 'appointment_booked');
      return {
        response: '‚úÖ Agendamento confirmado! Voc√™ receber√° um lembrete por email.',
        outcome: 'appointment_booked',
        newFlowLock: completedLock
      };
    }

    if (intent === 'slot_selection' && currentFlow === 'booking') {
      const nextLock = this.flowManager.advanceStep(context, 'confirm', { selectedSlot: messageText });
      return {
        response: `Confirma agendamento no hor√°rio ${messageText}? Digite "confirmo" para finalizar.`,
        outcome: 'booking_slot_selected',
        newFlowLock: nextLock
      };
    }

    // === FLUXOS OPERACIONAIS ===

    if (intent === 'booking') {
      if (!flowDecision.allow_intent) {
        return {
          response: flowDecision.suggested_response,
          outcome: 'booking_blocked_by_flow',
          newFlowLock: context.flow_lock
        };
      }

      const bookingLock = this.flowManager.startFlowLock('booking', 'collect_service');
      return {
        response: 'Perfeito! Para qual servi√ßo voc√™ gostaria de agendar?',
        outcome: 'booking_started',
        newFlowLock: bookingLock
      };
    }

    if (intent === 'reschedule') {
      const rescheduleLock = this.flowManager.startFlowLock('reschedule', 'collect_id');
      return {
        response: 'Vamos reagendar! Qual o ID do seu agendamento atual?',
        outcome: 'reschedule_started',
        newFlowLock: rescheduleLock
      };
    }

    if (intent === 'cancel') {
      const cancelLock = this.flowManager.startFlowLock('cancel', 'collect_id');
      return {
        response: 'Para cancelar, preciso do ID do agendamento. Qual √©?',
        outcome: 'cancel_started',
        newFlowLock: cancelLock
      };
    }

    if (intent === 'pricing') {
      const pricingLock = this.flowManager.startFlowLock('pricing', 'start');
      const response = this.generatePricingResponse(tenantConfig);
      return {
        response: response + ' Gostaria de agendar algum servi√ßo?',
        outcome: this.determineConversationOutcome('pricing', response), // ‚úÖ CORRE√á√ÉO: usar mapping correto
        newFlowLock: pricingLock
      };
    }

    if (intent === 'onboarding') {
      if (!flowDecision.allow_intent) {
        return {
          response: flowDecision.suggested_response,
          outcome: 'onboarding_blocked_by_flow',
          newFlowLock: context.flow_lock
        };
      }

      const onboardingLock = this.flowManager.startFlowLock('onboarding', 'collect_email');
      return {
        response: 'Ol√°! Para melhor atend√™-lo, qual seu email?',
        outcome: 'onboarding_started', 
        newFlowLock: onboardingLock
      };
    }

    // === INTENTS INSTITUCIONAIS (n√£o alteram fluxo) ===

    if (intent.startsWith('institutional_')) {
      const response = this.generateInstitutionalResponse(intent, tenantConfig);
      return {
        response,
        outcome: 'institutional_info_provided',
        newFlowLock: context.flow_lock // Manter fluxo atual
      };
    }

    // === FALLBACKS ===

    if (intent === 'greeting') {
      return {
        response: 'Ol√°! Como posso ajud√°-lo?',
        outcome: null as any, // üö® CORRE√á√ÉO: greeting n√£o finaliza conversa
        newFlowLock: null
      };
    }

    // General fallback - conversa ainda em andamento
    return {
      response: 'üö® FALLBACK: executeFlowAction - N√£o entendi. Pode reformular?',
      outcome: null as any, // üö® CORRE√á√ÉO: fallback n√£o finaliza conversa
      newFlowLock: context.flow_lock
    };
  }

  /**
   * Atualiza contexto com novo estado do flow
   */
  private async updateContextWithFlowState(
    userId: string,
    tenantId: string,
    currentContext: EnhancedConversationContext,
    newFlowLock: any,
    intentResult: any
  ): Promise<EnhancedConversationContext> {
    
    return await mergeEnhancedConversationContext(
      userId,
      tenantId,
      {
        ...currentContext,
        flow_lock: newFlowLock || currentContext.flow_lock
      },
      {
        intent: intentResult.intent,
        confidence: intentResult.confidence,
        decision_method: intentResult.decision_method
      }
    );
  }

  /**
   * Handlers especiais
   */
  
  private async handleExpiredFlow(context: EnhancedConversationContext, message: string): Promise<WebhookOrchestrationResult> {
    const cleanedContext = await mergeEnhancedConversationContext(
      'temp-user',
      context.tenant_id,
      { ...context, flow_lock: null }
    );
    
    return {
      aiResponse: message,
      shouldSendWhatsApp: true,
      conversationOutcome: 'timeout_expired',
      updatedContext: cleanedContext,
      telemetryData: {
        intent: 'timeout_expired',
        confidence: 1.0,
        decision_method: 'timeout',
        flow_lock_active: false,
        processing_time_ms: 0,
        model_used: 'timeout' // üöÄ Modelo de timeout para telemetry
      }
    };
  }

  private handleFlowWarning(context: EnhancedConversationContext, message: string): WebhookOrchestrationResult {
    return {
      aiResponse: message,
      shouldSendWhatsApp: true,
      conversationOutcome: 'timeout_warning',
      updatedContext: context,
      telemetryData: {
        intent: 'timeout_warning',
        confidence: 1.0,
        decision_method: 'timeout',
        flow_lock_active: !!context.flow_lock?.active_flow,
        processing_time_ms: 0,
        model_used: 'timeout_warning' // üöÄ Modelo de warning para telemetry
      }
    };
  }

  private handleBlockedIntent(context: EnhancedConversationContext, intentResult: any): WebhookOrchestrationResult {
    const currentFlow = context.flow_lock?.active_flow;
    const message = `Vamos terminar ${currentFlow} primeiro. Como posso continuar ajudando?`;
    
    return {
      aiResponse: message,
      shouldSendWhatsApp: true,
      conversationOutcome: 'intent_blocked_by_flow_lock',
      updatedContext: context,
      telemetryData: {
        intent: intentResult.intent,
        confidence: intentResult.confidence,
        decision_method: intentResult.decision_method,
        flow_lock_active: true,
        processing_time_ms: 0,
        model_used: 'blocked' // üöÄ Modelo de intent bloqueado para telemetry
      }
    };
  }

  /**
   * Geradores de resposta
   */
  
  private generatePricingResponse(tenantConfig: any): string {
    const services = tenantConfig?.services || [];
    if (services.length === 0) {
      return 'Entre em contato para informa√ß√µes sobre pre√ßos.';
    }

    let response = 'üí∞ Nossos pre√ßos:\n\n';
    services.slice(0, 5).forEach((service: any) => {
      response += `‚Ä¢ ${service.name}: R$ ${service.price}\n`;
    });
    
    return response;
  }

  private generateInstitutionalResponse(intent: string, tenantConfig: any): string {
    const policies = tenantConfig?.policies || {};
    
    const responses: Record<string, string> = {
      'institutional_address': policies.address || 'Consulte nosso site para endere√ßo.',
      'institutional_hours': policies.hours || 'Segunda a sexta, 8h √†s 18h.',
      'institutional_policy': policies.cancellation || 'Cancelamentos com 24h de anteced√™ncia.',
      'institutional_payment': 'Aceitamos dinheiro, cart√£o e PIX.',
      'institutional_contact': tenantConfig?.phone || 'Entre em contato pelo WhatsApp.'
    };

    return responses[intent] || 'Informa√ß√£o n√£o dispon√≠vel no momento.';
  }

  /**
   * üö® M√âTODO CR√çTICO: Gera resposta via OpenAI com contexto do Flow Lock
   * Flow Lock apenas gerencia estado, OpenAI gera TODAS as respostas
   */
  private async generateAIResponseWithFlowContext(
    messageText: string,
    intentResult: any,
    flowDecision: any,
    context: EnhancedConversationContext,
    tenantConfig: any
  ): Promise<{ response: string; outcome: string; newFlowLock?: any; llmMetrics?: any }> {
    
    const intent = intentResult.intent;
    const currentFlow: string | null = context.flow_lock?.active_flow || null;
    const currentStep: string | null = context.flow_lock?.step || null;

    // === COMANDOS DIRETOS COM FLOW LOCK (sem OpenAI) ===
    
    if (intent === 'flow_cancel') {
      const abandonedLock = this.flowManager.abandonFlow(context, 'user_requested');
      return {
        response: 'Cancelado. Como posso ajudar?',
        outcome: 'flow_cancelled',
        newFlowLock: abandonedLock
      };
    }

    if (intent === 'booking_confirm' && currentFlow === 'booking') {
      const completedLock = this.flowManager.completeFlow(context, 'appointment_booked');
      return {
        response: '‚úÖ Agendamento confirmado! Voc√™ receber√° um lembrete por email.',
        outcome: 'appointment_booked',
        newFlowLock: completedLock
      };
    }

    if (intent === 'slot_selection' && currentFlow === 'booking') {
      const nextLock = this.flowManager.advanceStep(context, 'confirm', { selectedSlot: messageText });
      return {
        response: `Confirma agendamento no hor√°rio ${messageText}? Digite "confirmo" para finalizar.`,
        outcome: 'booking_slot_selected',
        newFlowLock: nextLock
      };
    }

    // === USAR OPENAI PARA TODAS AS OUTRAS RESPOSTAS ===
    console.log('üö® M√âTODO CHAMADO: generateAIResponseWithFlowContext');
    console.log('üîç DEBUG - Entrando no bloco OpenAI:', { intent, currentFlow, currentStep });
    const startTime = Date.now();
    
    try {
      // Construir contexto para OpenAI
      const businessInfo = this.buildBusinessContext(tenantConfig);
      const flowContext = this.buildFlowContext(currentFlow, currentStep, intent);
      
      const systemPrompt = `Voc√™ √© a assistente oficial do ${tenantConfig.name || 'neg√≥cio'}. Seu papel √© atender com clareza, honestidade e objetividade, sempre em tom natural.

‚ö†Ô∏è REGRAS DE HONESTIDADE ABSOLUTA - OBRIGAT√ìRIAS:
- NUNCA invente dados. NUNCA prometa retorno. NUNCA mencione atendente humano.
- Para informa√ß√µes inexistentes (endere√ßo, hor√°rios, pagamento, pol√≠ticas) use SEMPRE a frase exata: "Infelizmente neste momento n√£o possuo esta informa√ß√£o no sistema."
- Use APENAS dados reais do sistema. Zero inven√ß√µes. Zero promessas.
- PROIBIDO inventar: hor√°rios, endere√ßos, formas de pagamento, telefones, pol√≠ticas.

${businessInfo}

üéØ DADOS PERMITIDOS (somente se existirem):
- Servi√ßos com pre√ßos reais
- Agendamentos confirmados  
- Profissionais cadastrados

üö´ DADOS PROIBIDOS (sempre usar frase padr√£o):
- Hor√°rios de funcionamento
- Endere√ßo/localiza√ß√£o  
- Formas de pagamento
- Contatos telef√¥nicos
- Pol√≠ticas n√£o confirmadas

IMPORTANTE: Responda APENAS com a mensagem honesta para o cliente. Se n√£o souber, use exatamente: "Infelizmente neste momento n√£o possuo esta informa√ß√£o no sistema."`;

      const userPrompt = `Mensagem do cliente: "${messageText}"
Inten√ß√£o detectada: ${intent}`;

      console.log('ü§ñ Chamando OpenAI com fallback escalonado...');
      
      // Fun√ß√£o para calcular confidence do modelo
      const calculateConfidence = (completion: any): number => {
        try {
          if (!completion.choices[0]?.logprobs?.content) {
            const finishReason = completion.choices[0]?.finish_reason;
            switch (finishReason) {
              case 'stop': return 0.85;
              case 'length': return 0.60;
              case 'content_filter': return 0.30;
              default: return 0.50;
            }
          }

          const logprobs = completion.choices[0].logprobs.content;
          if (!logprobs || logprobs.length === 0) return 0.70;

          const avgLogprob = logprobs.reduce((sum: number, token: any) => {
            return sum + (token.logprob || -2.0);
          }, 0) / logprobs.length;

          const confidence = Math.max(0.1, Math.min(0.99, Math.exp(avgLogprob / -2.0)));
          return Math.round(confidence * 100) / 100;
        } catch (error) {
          return 0.70;
        }
      };
      
      // Sistema de fallback escalonado usando configura√ß√£o centralizada
      // üöÄ CORRE√á√ÉO: Ordem correta por custo (mais barato ‚Üí mais caro)
      // gpt-4o-mini ($0.00075) ‚Üí gpt-3.5-turbo ($0.0035) ‚Üí gpt-4 ($0.09)
      const models = [MODELS.FAST, MODELS.BALANCED, MODELS.STRICT] as const;
      let completion: any = null;
      let modelUsed: string = MODELS.FAST; // Come√ßar com o mais barato
      let finalConfidenceScore = 0;
      
      for (let i = 0; i < models.length; i++) {
        const model = models[i];
        
        try {
          console.log(`üéØ Tentando modelo: ${model}`);
          
          completion = await this.openai.chat.completions.create({
            model: model as any,
            temperature: 0.7,
            max_tokens: 300,
            logprobs: true,
            top_logprobs: 3,
            messages: [
              { role: 'system', content: systemPrompt },
              { role: 'user', content: userPrompt }
            ]
          });
          
          const text = completion.choices?.[0]?.message?.content ?? '';
          
          // üìä Calcular confidence score apenas para m√©tricas (n√£o para fallback)
          finalConfidenceScore = calculateConfidence(completion);
          console.log(`üìä Confidence ${model}: ${finalConfidenceScore}`);
          
          // üîç Valida√ß√£o objetiva da resposta (substitui threshold artificial)
          const { valid, validationScore } = this.validateAIResponse(text, messageText, intentResult);
          
          if (valid || i === models.length - 1) {
            modelUsed = model as string;
            console.log(`‚úÖ Modelo escolhido: ${model} (validation: ${valid ? 'PASS' : 'FAIL-LAST'}, confidence: ${finalConfidenceScore})`);
            
            // üöÄ CR√çTICO: Capturar m√©tricas apenas do modelo vencedor
            const usage = completion.usage || {};
            const apiCost = this.calculateOpenAICost(usage, model);
            
            console.log(`üí∞ M√©tricas do modelo vencedor: tokens=${usage.total_tokens}, cost=${apiCost}, model=${model}`);
            break;
          }
          
          console.log(`‚ö†Ô∏è Resposta n√£o passou na valida√ß√£o, tentando pr√≥ximo modelo...`);
          
        } catch (error: any) {
          console.error(`‚ùå Erro no modelo ${model}:`, {
            message: error?.message || 'Erro desconhecido',
            status: error?.status || error?.response?.status,
            statusText: error?.response?.statusText,
            code: error?.code,
            type: error?.type,
            param: error?.param,
            errorDetails: error?.error || error?.response?.data,
            stack: error?.stack?.substring(0, 500) // Truncar stack trace
          });
          
          // Detectar tipos espec√≠ficos de erro da OpenAI API
          if (error?.status === 400) {
            console.log(`üö® API Error 400 (Bad Request) - Poss√≠vel problema com par√¢metros do modelo ${model}`);
          } else if (error?.status === 401) {
            console.log(`üö® API Error 401 (Unauthorized) - Problema de autentica√ß√£o`);
          } else if (error?.status === 403) {
            console.log(`üö® API Error 403 (Forbidden) - Modelo ${model} pode n√£o estar dispon√≠vel para sua conta`);
          } else if (error?.status === 429) {
            console.log(`üö® API Error 429 (Rate Limit) - Rate limit atingido no modelo ${model}`);
          } else if (error?.status === 500 || error?.status === 502 || error?.status === 503) {
            console.log(`üö® API Error ${error.status} - Erro interno da OpenAI no modelo ${model}`);
          } else if (error?.code === 'ENOTFOUND' || error?.code === 'ETIMEDOUT') {
            console.log(`üö® Network Error - Problema de conectividade com OpenAI (${error.code})`);
          } else {
            console.log(`üö® Erro n√£o identificado no modelo ${model} - Type: ${error?.type || 'unknown'}`);
          }
          
          if (i === models.length - 1) {
            throw error; // Re-throw no √∫ltimo modelo
          }
          console.log(`üîÑ Tentando pr√≥ximo modelo...`);
        }
      }

      // Verificar se temos uma completion v√°lida
      if (!completion) {
        throw new Error('Nenhum modelo conseguiu gerar resposta');
      }

      const aiResponse = completion.choices[0]?.message?.content?.trim() || 'üö® FALLBACK: generateAIResponseWithFlowContext - N√£o entendi. Pode reformular?';
      const latencyMs = Date.now() - startTime;

      // Usar a fun√ß√£o de confidence j√° definida
      const aiConfidenceScore = calculateConfidence(completion);
      console.log(`üîç Final confidence score: ${aiConfidenceScore} com modelo: ${modelUsed}`);

      // ‚úÖ CAPTURAR M√âTRICAS LLM COM VALIDA√á√ÉO
      const usage = completion.usage;
      const apiCost = this.calculateOpenAICost(usage, modelUsed);
      
      // ‚úÖ VALIDA√á√ÉO: Verificar se usage est√° dispon√≠vel
      if (!usage || !usage.total_tokens) {
        console.warn('‚ö†Ô∏è OpenAI usage data n√£o dispon√≠vel', { usage, completion });
      }
      
      // ‚úÖ CALCULAR PROCESSING COST de forma mais realista
      const calculateProcessingCost = (apiCost: number | null): number => {
        if (!apiCost) return 0.00001; // Custo m√≠nimo para opera√ß√µes sem API
        
        // ‚úÖ L√ìGICA REALISTA: 10% do custo da API + custo fixo de infraestrutura
        const percentageCost = apiCost * 0.10; // 10% do custo da API
        const infrastructureCost = 0.00002; // Custo fixo de infraestrutura (Supabase, Redis, etc.)
        const databaseCost = 0.00001; // Custo de 2 INSERTs na conversation_history
        
        return Math.round((percentageCost + infrastructureCost + databaseCost) * 100000) / 100000;
      };
      
      const llmMetrics = {
        prompt_tokens: usage?.prompt_tokens ?? null,
        completion_tokens: usage?.completion_tokens ?? null,
        total_tokens: usage?.total_tokens ?? null,
        api_cost_usd: apiCost,
        processing_cost_usd: (() => {
          if (!apiCost) return 0.00001; // custo m√≠nimo
          const percentageCost = apiCost * 0.10; // 10% do custo da API
          const infrastructureCost = 0.00002; // custo fixo de infraestrutura
          const databaseCost = 0.00001; // custo de inserts no conversation_history
          return Math.round((apiCost + percentageCost + infrastructureCost + databaseCost) * 100000) / 100000;
        })(),
        confidence_score: aiConfidenceScore,
        latency_ms: latencyMs,
        model_used: modelUsed // üöÄ Incluir modelo usado no fallback
      };

      console.log('‚úÖ OpenAI respondeu:', {
        intent,
        model: modelUsed,
        tokens: usage?.total_tokens,
        confidence: aiConfidenceScore,
        latency: latencyMs,
        cost: llmMetrics.api_cost_usd
      });

      // Determinar novo estado do Flow Lock baseado na inten√ß√£o
      const newFlowLock = this.determineNewFlowState(intent, currentFlow, context);
      
      // üö® CORRE√á√ÉO CR√çTICA: Outcome ser√° determinado pelo shouldPersistOutcome, n√£o aqui
      // generateAIResponseWithFlowContext n√£o deve determinar outcomes
      const outcome = null as any; // AI responses n√£o finalizam conversas por si s√≥

      return {
        response: aiResponse,
        outcome,
        newFlowLock,
        llmMetrics
      };

    } catch (error: any) {
      console.error('‚ùå Erro ao chamar OpenAI:', error);
      console.error('‚ùå Stack trace:', error?.stack);
      console.error('‚ùå Error details:', { message: error?.message, code: error?.code, status: error?.status });
      
      // Fallback para resposta determin√≠stica
      return await this.executeFlowAction(
        messageText,
        intentResult,
        flowDecision,
        context,
        tenantConfig
      );
    }
  }

  /**
   * Constr√≥i contexto do neg√≥cio para OpenAI
   */
  private buildBusinessContext(tenantConfig: any): string {
    const services = tenantConfig?.services?.slice(0, 5) || [];
    const policies = tenantConfig?.policies || {};
    
    let context = `SOBRE O NEG√ìCIO:
- Nome: ${tenantConfig.name || 'N√£o informado'}
- Tipo: ${tenantConfig.domain || 'Servi√ßos gerais'}`;

    if (services.length > 0) {
      context += `\n- Servi√ßos: ${services.map((s: any) => `${s.name} (R$ ${s.price})`).join(', ')}`;
    }

    if (policies.address) {
      context += `\n- Endere√ßo: ${policies.address}`;
    }

    if (policies.hours) {
      context += `\n- Hor√°rio: ${policies.hours}`;
    }

    return context;
  }

  /**
   * Constr√≥i contexto do fluxo atual para OpenAI
   */
  private buildFlowContext(currentFlow: string | null, currentStep: string | null, intent: string | null): string {
    if (!currentFlow) {
      return `O cliente est√° iniciando uma nova conversa. Inten√ß√£o detectada: ${intent}`;
    }

    return `O cliente est√° no meio de um fluxo de ${currentFlow}, etapa: ${currentStep || 'inicial'}. Inten√ß√£o detectada: ${intent}`;
  }

  /**
   * Calcula custo estimado da chamada OpenAI
   */
  private calculateOpenAICost(usage: any, model?: string): number | null {
    if (!usage || !usage.prompt_tokens || !usage.completion_tokens) {
      return null;
    }

    // üí∞ CUSTOS CORRETOS POR MODELO (por 1K tokens)
    const modelCosts: Record<string, { prompt: number; completion: number }> = {
      'gpt-4o-mini': { prompt: 0.00015, completion: 0.0006 },
      'gpt-3.5-turbo': { prompt: 0.0015, completion: 0.002 },
      'gpt-4': { prompt: 0.03, completion: 0.06 },
      'gpt-4o': { prompt: 0.005, completion: 0.015 }
    };

    // Detectar modelo atual ou usar fallback gen√©rico
    const costs = modelCosts[model || 'gpt-4'] || modelCosts['gpt-4'];
    
    const promptCost = (usage.prompt_tokens / 1000) * costs!.prompt;
    const completionCost = (usage.completion_tokens / 1000) * costs!.completion;
    
    return Math.round((promptCost + completionCost) * 1000000) / 1000000; // 6 casas decimais para precis√£o
  }

  /**
   * Valida resposta do AI usando crit√©rios simples e objetivos
   * üöÄ CORRE√á√ÉO: Se tem resposta v√°lida, aceita. Se n√£o tem, escala.
   */
  private validateAIResponse(text: string, userMessage: string, intentResult: any): { valid: boolean; validationScore: number } {
    // ‚úÖ VALIDA√á√ÉO SIMPLES: Resposta n√£o-vazia = v√°lida
    const hasValidResponse = !!(text && text.trim().length > 0);
    
    // ‚úÖ CHECK B√ÅSICO: N√£o cont√©m padr√µes √≥bvios de erro
    const errorPatterns = [
      /\[erro\]/i,
      /\[error\]/i,
      /undefined/i,
      /null$/i,
      /^error:/i,
      /^failed:/i
    ];
    
    const hasErrorPattern = errorPatterns.some(pattern => pattern.test(text));
    
    // ‚úÖ L√ìGICA SIMPLES: Tem resposta v√°lida E n√£o tem erro = aceita
    const valid = hasValidResponse && !hasErrorPattern;
    const validationScore = valid ? 1.0 : 0.0;

    return { valid, validationScore };
  }

  /**
   * Determina novo estado do Flow Lock baseado na inten√ß√£o
   */
  private determineNewFlowState(intent: string | null, currentFlow: string | null, context: EnhancedConversationContext): any {
    const targetFlow = this.mapIntentToFlow(intent);
    
    if (!targetFlow) {
      return context.flow_lock; // Manter estado atual
    }

    // Iniciar novo fluxo se n√£o h√° fluxo ativo
    if (!currentFlow) {
      return this.flowManager.startFlowLock(targetFlow, 'start');
    }

    // Continuar fluxo atual
    return context.flow_lock;
  }

  /**
   * Detecta se a conversa est√° finalizada e deve persistir outcome
   * Retorna null se conversa ainda est√° em andamento
   */
  private shouldPersistOutcome(intent: string | null, response: string, context: EnhancedConversationContext): string | null {
    // üö® CORRE√á√ÉO CR√çTICA: Outcome deve ser NULL para conversas em andamento
    // S√≥ persistir quando conversa REALMENTE finaliza
    
    // 1. INTENTS FINALIZADORES EXPL√çCITOS - apenas confirma√ß√µes que criam/modificam appointments
    const trulyFinalizingIntents = [
      'booking_confirm', 'cancel_confirm', 'reschedule_confirm'
    ];
    
    if (intent && trulyFinalizingIntents.includes(intent)) {
      return this.determineConversationOutcome(intent, response);
    }
    
    // 2. TIMEOUT DE INATIVIDADE - s√≥ ser√° processado pelo cronjob, n√£o aqui
    // Removida l√≥gica de timeout pois ser√° handled pelo ConversationOutcomeProcessor
    
    // 3. TODAS AS OUTRAS INTERA√á√ïES - conversa ainda em andamento
    // Includes: greeting, pricing, booking, address, business_hours, etc.
    // Essas s√£o intera√ß√µes dentro da conversa, n√£o o fim dela
    return null;
  }

  /**
   * Determina outcome da conversa baseado na inten√ß√£o e resposta
   * APENAS para intents que realmente finalizam conversa
   */
  private determineConversationOutcome(intent: string | null, response: string): string {
    // Mapear APENAS intents finalizadores para outcomes v√°lidos
    const finalizingOutcomeMap: Record<string, string> = {
      'booking_confirm': 'appointment_created',
      'cancel_confirm': 'appointment_cancelled', 
      'reschedule_confirm': 'appointment_modified'
    };

    // Se n√£o √© um intent finalizador, isso √© um erro de l√≥gica
    if (!intent || !finalizingOutcomeMap[intent]) {
      console.error(`üö® ERRO: determineConversationOutcome chamado para intent n√£o-finalizador: ${intent}`);
      return 'error';
    }

    return finalizingOutcomeMap[intent];
  }

  /**
   * DETECTAR OUTCOME QUANDO CONVERSA FINALIZA
   * APENAS an√°lise - persist√™ncia √© responsabilidade do cronjob
   */
  async checkAndPersistConversationOutcome(
    sessionId: string,
    trigger: 'timeout' | 'flow_completion' | 'appointment_action' | 'user_exit' = 'flow_completion'
  ): Promise<void> {
    try {
      // APENAS analisar - SEM persistir (responsabilidade do cronjob)
      const analysis = await this.outcomeAnalyzer.analyzeConversationOutcome(sessionId, trigger);
      
      if (analysis) {
        console.log(`üéØ Conversation outcome analyzed: ${analysis.outcome} (${analysis.confidence}) - will be persisted by cronjob`);
      }
    } catch (error) {
      console.error('‚ùå Failed to check conversation outcome:', error);
    }
  }

  /**
   * EXECUTAR AN√ÅLISE PERI√ìDICA DE CONVERSAS FINALIZADAS
   * Deve ser chamado por cronjob para processar conversas abandonadas por timeout
   */
  async processFinishedConversations(): Promise<void> {
    try {
      await this.outcomeAnalyzer.checkForFinishedConversations();
    } catch (error) {
      console.error('‚ùå Failed to process finished conversations:', error);
    }
  }

  /**
   * üö® FUN√á√ÉO REMOVIDA: tryDeterministicResponse
   * Todas as respostas devem usar OpenAI para capturar m√©tricas LLM corretas
   */

  /**
   * CLASSIFICA√á√ÉO DE INTENTS COM SISTEMA DE FALLBACK LLM
   * Usa o mesmo sistema de fallback: 3.5-turbo ‚Üí 4o-mini ‚Üí 4
   */
  private async classifyIntentWithLLMFallback(messageText: string): Promise<{ intent: string | null; confidence: number } | null> {
    console.log('üéØ Iniciando classifica√ß√£o de intent com fallback LLM...');

    // Intents permitidos (baseado no ai-complex.service.js original)
    const ALLOWED_INTENTS = [
      'greeting', 'booking', 'pricing',
      'address', 'business_hours', 'services',
      'flow_cancel', 'my_appointments'
    ];

    const systemPrompt = `Classifique a inten√ß√£o do usu√°rio usando APENAS as op√ß√µes abaixo:

${ALLOWED_INTENTS.join(', ')}

Instru√ß√µes:
- Responda APENAS com o nome da inten√ß√£o
- Se n√£o se encaixar em nenhuma categoria, n√£o responda nada
- Seja preciso e conciso

Exemplos:
- "Ol√°" ‚Üí greeting
- "Quero marcar" ‚Üí booking  
- "Quanto custa?" ‚Üí pricing
- "Onde voc√™s ficam?" ‚Üí address
- "Que horas abrem?" ‚Üí business_hours
- "Quais servi√ßos?" ‚Üí services
- "Cancelar" ‚Üí flow_cancel
- "Meus agendamentos" ‚Üí my_appointments

Op√ß√µes v√°lidas: ${ALLOWED_INTENTS.join(', ')}`;

    // Sistema de fallback usando configura√ß√£o centralizada
    // üöÄ CORRE√á√ÉO: Ordem correta por custo (mais barato ‚Üí mais caro)
    // gpt-4o-mini ($0.00075) ‚Üí gpt-3.5-turbo ($0.0035) ‚Üí gpt-4 ($0.09)
    const models = [MODELS.FAST, MODELS.BALANCED, MODELS.STRICT] as const;
    
    for (let i = 0; i < models.length; i++) {
      const model = models[i];
      
      try {
        console.log(`üéØ Tentando classificar intent com modelo: ${model}`);
        
        const completion = await this.openai.chat.completions.create({
          model: model as any,
          temperature: 0,
          max_tokens: 8,
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: messageText }
          ]
        });

        const raw = (completion.choices?.[0]?.message?.content || '').trim().toLowerCase();
        const intent = raw.replace(/[^a-z_]/g, ''); // sanitiza

        // üîç Valida√ß√£o objetiva: intent deve estar na lista permitida
        const isValidIntent = ALLOWED_INTENTS.includes(intent as any);
        
        if (isValidIntent || i === models.length - 1) {
          // Calcular confidence apenas para m√©tricas (n√£o para fallback)
          const confidence = intent === 'general' ? 0.5 : 0.8;
          
          console.log(`‚úÖ Intent classificado: ${intent} (modelo: ${model}, valid: ${isValidIntent ? 'PASS' : 'FAIL-LAST'}, confidence: ${confidence})`);
          
          if (isValidIntent) {
            // üöÄ CR√çTICO: Log do modelo vencedor para classifica√ß√£o
            console.log(`üí° Intent classificado pelo modelo vencedor: ${model} (${intent}, confidence: ${confidence})`);
            return { intent, confidence };
          } else {
            // √öltimo modelo e intent inv√°lido
            console.log(`‚ùå √öltimo modelo ${model} retornou intent inv√°lido: ${intent}`);
            return null;
          }
        }
        
        console.log(`‚ö†Ô∏è Intent inv√°lido: ${intent}, tentando pr√≥ximo modelo...`)
        
      } catch (error: any) {
        console.error(`‚ùå Erro no modelo ${model} para classifica√ß√£o de intent:`, {
          message: error?.message || 'Erro desconhecido',
          status: error?.status || error?.response?.status,
          statusText: error?.response?.statusText,
          code: error?.code,
          type: error?.type,
          param: error?.param,
          errorDetails: error?.error || error?.response?.data
        });
        
        // Detectar tipos espec√≠ficos de erro da OpenAI API para classifica√ß√£o
        if (error?.status === 403) {
          console.log(`üö® CLASSIFICA√á√ÉO - Modelo ${model} pode n√£o estar dispon√≠vel (Error 403)`);
        } else if (error?.status === 429) {
          console.log(`üö® CLASSIFICA√á√ÉO - Rate limit atingido no modelo ${model}`);
        } else if (error?.status === 400) {
          console.log(`üö® CLASSIFICA√á√ÉO - Par√¢metros inv√°lidos para modelo ${model}`);
        }
        
        if (i === models.length - 1) {
          throw error; // Re-throw no √∫ltimo modelo
        }
        console.log(`üîÑ Tentando pr√≥ximo modelo para classifica√ß√£o...`);
      }
    }

    return null;
  }

}