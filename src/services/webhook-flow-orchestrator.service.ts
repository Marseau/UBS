/**
 * Webhook Flow Orchestrator Service
 * Orquestra integra√ß√£o do Flow Lock System com webhook existente
 * Baseado na OS: Sincronizar Inten√ß√µes e Evitar Mistura
 */

import { DeterministicIntentDetectorService, detectIntents, INTENT_KEYS } from './deterministic-intent-detector.service';
import { FlowLockManagerService } from './flow-lock-manager.service';
import { ConversationOutcomeAnalyzerService } from './conversation-outcome-analyzer.service';
import { mergeEnhancedConversationContext } from '../utils/conversation-context-helper';
import { EnhancedConversationContext, FlowType, FlowStep } from '../types/flow-lock.types';
import OpenAI from 'openai';

export interface WebhookOrchestrationResult {
  aiResponse: string;
  shouldSendWhatsApp: boolean;
  conversationOutcome: string | null; // null se conversa em andamento
  updatedContext: EnhancedConversationContext;
  telemetryData: {
    intent: string | null;
    confidence: number;
    decision_method: string;
    flow_lock_active: boolean;
    processing_time_ms: number;
  };
  llmMetrics?: {
    prompt_tokens: number | null;
    completion_tokens: number | null;
    total_tokens: number | null;
    api_cost_usd: number | null;
    processing_cost_usd: number | null;
    confidence_score: number | null;
    latency_ms: number | null;
  };
}

export class WebhookFlowOrchestratorService {
  private intentDetector: DeterministicIntentDetectorService;
  private flowManager: FlowLockManagerService;
  private outcomeAnalyzer: ConversationOutcomeAnalyzerService;
  private openai: OpenAI;

  constructor() {
    this.intentDetector = new DeterministicIntentDetectorService();
    this.flowManager = new FlowLockManagerService();
    this.outcomeAnalyzer = new ConversationOutcomeAnalyzerService();
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY || ''
    });
  }

  /**
   * Processamento principal do webhook com Flow Lock
   */
  async orchestrateWebhookFlow(
    messageText: string,
    userId: string,
    tenantId: string,
    tenantConfig: any,
    existingContext?: any
  ): Promise<WebhookOrchestrationResult> {
    const startTime = Date.now();

    try {
      // 1. Resolver contexto enhanced (com flow_lock)
      const context = await this.resolveEnhancedContext(
        userId, 
        tenantId, 
        tenantConfig,
        existingContext
      );

      // 2. Verificar timeout de fluxo ativo
      const timeoutStatus = this.flowManager.checkTimeoutStatus(context);
      if (timeoutStatus.status === 'expired') {
        return await this.handleExpiredFlow(context, timeoutStatus.message || '');
      }
      if (timeoutStatus.status === 'warning') {
        return this.handleFlowWarning(context, timeoutStatus.message || '');
      }

      // 3. Detec√ß√£o determin√≠stica de inten√ß√£o
      const primary = this.intentDetector.detectPrimaryIntent(messageText); // string | null

      // üìä LOG: ap√≥s regex (camada 1)
      console.log('[INTENT] regex primary:', primary);

      let finalIntent: string | null = primary;

      if (!finalIntent) {
        // üìä LOG: antes de chamar LLM (camada 2)
        console.log('[INTENT] Calling LLM (regex=null)');
        finalIntent = await this.classifyIntentWithLLM(messageText);
      }

      // üìä LOG: ap√≥s LLM (camada 2)  
      console.log('[INTENT] final:', finalIntent);

      // se ainda null ‚Üí desambigua√ß√£o (camada 3)
      if (!finalIntent) {
        // responder pergunta curta e marcar awaiting_intent = true
        // ...
      }
      
      // ‚úÖ ADAPTER: Converter para formato esperado pelo resto do c√≥digo
      const intentResult = {
        intent: finalIntent,
        confidence: finalIntent ? 0.95 : 0.0,
        decision_method: finalIntent === primary ? 'deterministic_regex' : 'llm_classification',
        allowed_by_flow_lock: true
      } as const;

      // üö® CORRE√á√ÉO: SEMPRE usar OpenAI para gerar respostas reais e capturar m√©tricas LLM
      // Removido curto-circuito determin√≠stico que impedia uso do OpenAI

      // 4. Verificar se inten√ß√£o √© permitida pelo flow lock
      if (!intentResult.allowed_by_flow_lock) {
        return this.handleBlockedIntent(context, intentResult);
      }

      // 5. Determinar novo fluxo baseado na inten√ß√£o
      const targetFlow = this.mapIntentToFlow(intentResult.intent);
      const flowDecision = this.flowManager.canStartNewFlow(context, targetFlow);

      // 6. üö® CORRE√á√ÉO CR√çTICA: Sempre usar OpenAI para gerar resposta real
      // Flow Lock apenas gerencia estado, OpenAI gera TODAS as respostas
      console.log('üîç ORQUESTRADOR DEBUG - Antes de chamar generateAIResponseWithFlowContext:', {
        intent: intentResult.intent,
        flowLock: context.flow_lock?.active_flow,
        messageText: messageText.substring(0, 50)
      });
      
      const result = await this.generateAIResponseWithFlowContext(
        messageText,
        intentResult,
        flowDecision,
        context,
        tenantConfig
      );
      
      console.log('üîç ORQUESTRADOR DEBUG - Resultado generateAIResponseWithFlowContext:', {
        hasLLMMetrics: !!result.llmMetrics,
        outcome: result.outcome,
        responseLength: result.response.length
      });

      // 7. Atualizar contexto com novo estado
      const updatedContext = await this.updateContextWithFlowState(
        userId,
        tenantId,
        context,
        result.newFlowLock,
        intentResult
      );

      // üö® CORRE√á√ÉO: S√≥ persistir outcome se conversa finalizada
      const finalOutcome = this.shouldPersistOutcome(intentResult.intent, result.response, updatedContext);
      console.log('üîß OUTCOME PERSISTENCE CHECK:', {
        intent: intentResult.intent,
        shouldPersist: finalOutcome !== null,
        outcome: finalOutcome
      });

      return {
        aiResponse: result.response,
        shouldSendWhatsApp: true,
        conversationOutcome: finalOutcome, // null se conversa em andamento
        llmMetrics: result.llmMetrics, // üö® CORRE√á√ÉO: Incluir m√©tricas LLM no retorno
        updatedContext,
        telemetryData: {
          intent: intentResult.intent,
          confidence: intentResult.confidence,
          decision_method: intentResult.decision_method,
          flow_lock_active: !!updatedContext.flow_lock?.active_flow,
          processing_time_ms: Date.now() - startTime
        }
      };

    } catch (error) {
      console.error('Webhook orchestration error:', error);
      
      // Fallback para sistema legado
      return {
        aiResponse: 'Desculpe, ocorreu um erro. Tente novamente.',
        shouldSendWhatsApp: true,
        conversationOutcome: 'error',
        updatedContext: await this.resolveEnhancedContext(userId, tenantId, tenantConfig, existingContext),
        telemetryData: {
          intent: 'error',
          confidence: 0,
          decision_method: 'error',
          flow_lock_active: false,
          processing_time_ms: Date.now() - startTime
        },
        llmMetrics: {
          prompt_tokens: null,
          completion_tokens: null,
          total_tokens: null,
          api_cost_usd: null,
          processing_cost_usd: null,
          confidence_score: null,
          latency_ms: null
        }
      };
    }
  }

  /**
   * Resolve contexto enhanced (backward compatible)
   */
  private async resolveEnhancedContext(
    userId: string,
    tenantId: string,
    tenantConfig: any,
    existingContext?: any
  ): Promise<EnhancedConversationContext> {
    
    const baseUpdates = {
      tenant_id: tenantId,
      domain: tenantConfig.domain || 'general',
      source: 'whatsapp' as const,
      mode: 'prod' as const
    };

    // Se j√° temos contexto legado, converter para enhanced
    if (existingContext) {
      return await mergeEnhancedConversationContext(
        userId,
        tenantId,
        { ...existingContext, ...baseUpdates }
      );
    }

    // Criar novo contexto enhanced
    return await mergeEnhancedConversationContext(
      userId,
      tenantId,
      baseUpdates
    );
  }

  /**
   * Mapeia intent para flow type
   */
  private mapIntentToFlow(intent: string | null): FlowType {
    if (!intent) return null;
    const flowMap: Record<string, FlowType> = {
      'onboarding': 'onboarding',
      'booking': 'booking',
      'booking_confirm': 'booking',
      'slot_selection': 'booking',
      'reschedule': 'reschedule',
      'reschedule_confirm': 'reschedule', 
      'cancel': 'cancel',
      'cancel_confirm': 'cancel',
      'pricing': 'pricing',
      'services': 'pricing',
      'flow_cancel': null,
      'greeting': null,
      'general': null
    };

    // Intents institucionais n√£o criam fluxo
    if (intent.startsWith('institutional_')) return null;

    return flowMap[intent] || null;
  }

  /**
   * Executa a√ß√£o do fluxo baseada na decis√£o
   */
  private async executeFlowAction(
    messageText: string,
    intentResult: any,
    flowDecision: any,
    context: EnhancedConversationContext,
    tenantConfig: any
  ): Promise<{ response: string; outcome: string; newFlowLock?: any }> {

    const intent = intentResult.intent;
    const currentFlow = context.flow_lock?.active_flow;
    const currentStep = context.flow_lock?.step;

    // === COMANDO DIRETOS (prioridade m√°xima) ===
    
    if (intent === 'flow_cancel') {
      const abandonedLock = this.flowManager.abandonFlow(context, 'user_requested');
      return {
        response: 'Cancelado. Como posso ajudar?',
        outcome: 'flow_cancelled',
        newFlowLock: abandonedLock
      };
    }

    if (intent === 'booking_confirm' && currentFlow === 'booking') {
      const completedLock = this.flowManager.completeFlow(context, 'appointment_booked');
      return {
        response: '‚úÖ Agendamento confirmado! Voc√™ receber√° um lembrete por email.',
        outcome: 'appointment_booked',
        newFlowLock: completedLock
      };
    }

    if (intent === 'slot_selection' && currentFlow === 'booking') {
      const nextLock = this.flowManager.advanceStep(context, 'confirm', { selectedSlot: messageText });
      return {
        response: `Confirma agendamento no hor√°rio ${messageText}? Digite "confirmo" para finalizar.`,
        outcome: 'booking_slot_selected',
        newFlowLock: nextLock
      };
    }

    // === FLUXOS OPERACIONAIS ===

    if (intent === 'booking') {
      if (!flowDecision.allow_intent) {
        return {
          response: flowDecision.suggested_response,
          outcome: 'booking_blocked_by_flow',
          newFlowLock: context.flow_lock
        };
      }

      const bookingLock = this.flowManager.startFlowLock('booking', 'collect_service');
      return {
        response: 'Perfeito! Para qual servi√ßo voc√™ gostaria de agendar?',
        outcome: 'booking_started',
        newFlowLock: bookingLock
      };
    }

    if (intent === 'reschedule') {
      const rescheduleLock = this.flowManager.startFlowLock('reschedule', 'collect_id');
      return {
        response: 'Vamos reagendar! Qual o ID do seu agendamento atual?',
        outcome: 'reschedule_started',
        newFlowLock: rescheduleLock
      };
    }

    if (intent === 'cancel') {
      const cancelLock = this.flowManager.startFlowLock('cancel', 'collect_id');
      return {
        response: 'Para cancelar, preciso do ID do agendamento. Qual √©?',
        outcome: 'cancel_started',
        newFlowLock: cancelLock
      };
    }

    if (intent === 'pricing') {
      const pricingLock = this.flowManager.startFlowLock('pricing', 'start');
      const response = this.generatePricingResponse(tenantConfig);
      return {
        response: response + ' Gostaria de agendar algum servi√ßo?',
        outcome: this.determineConversationOutcome('pricing', response), // ‚úÖ CORRE√á√ÉO: usar mapping correto
        newFlowLock: pricingLock
      };
    }

    if (intent === 'onboarding') {
      if (!flowDecision.allow_intent) {
        return {
          response: flowDecision.suggested_response,
          outcome: 'onboarding_blocked_by_flow',
          newFlowLock: context.flow_lock
        };
      }

      const onboardingLock = this.flowManager.startFlowLock('onboarding', 'collect_email');
      return {
        response: 'Ol√°! Para melhor atend√™-lo, qual seu email?',
        outcome: 'onboarding_started', 
        newFlowLock: onboardingLock
      };
    }

    // === INTENTS INSTITUCIONAIS (n√£o alteram fluxo) ===

    if (intent.startsWith('institutional_')) {
      const response = this.generateInstitutionalResponse(intent, tenantConfig);
      return {
        response,
        outcome: 'institutional_info_provided',
        newFlowLock: context.flow_lock // Manter fluxo atual
      };
    }

    // === FALLBACKS ===

    if (intent === 'greeting') {
      return {
        response: 'Ol√°! Como posso ajud√°-lo?',
        outcome: null as any, // üö® CORRE√á√ÉO: greeting n√£o finaliza conversa
        newFlowLock: null
      };
    }

    // General fallback - conversa ainda em andamento
    return {
      response: 'üö® FALLBACK: executeFlowAction - N√£o entendi. Pode reformular?',
      outcome: null as any, // üö® CORRE√á√ÉO: fallback n√£o finaliza conversa
      newFlowLock: context.flow_lock
    };
  }

  /**
   * Atualiza contexto com novo estado do flow
   */
  private async updateContextWithFlowState(
    userId: string,
    tenantId: string,
    currentContext: EnhancedConversationContext,
    newFlowLock: any,
    intentResult: any
  ): Promise<EnhancedConversationContext> {
    
    return await mergeEnhancedConversationContext(
      userId,
      tenantId,
      {
        ...currentContext,
        flow_lock: newFlowLock || currentContext.flow_lock
      },
      {
        intent: intentResult.intent,
        confidence: intentResult.confidence,
        decision_method: intentResult.decision_method
      }
    );
  }

  /**
   * Handlers especiais
   */
  
  private async handleExpiredFlow(context: EnhancedConversationContext, message: string): Promise<WebhookOrchestrationResult> {
    const cleanedContext = await mergeEnhancedConversationContext(
      'temp-user',
      context.tenant_id,
      { ...context, flow_lock: null }
    );
    
    return {
      aiResponse: message,
      shouldSendWhatsApp: true,
      conversationOutcome: 'timeout_expired',
      updatedContext: cleanedContext,
      telemetryData: {
        intent: 'timeout_expired',
        confidence: 1.0,
        decision_method: 'timeout',
        flow_lock_active: false,
        processing_time_ms: 0
      }
    };
  }

  private handleFlowWarning(context: EnhancedConversationContext, message: string): WebhookOrchestrationResult {
    return {
      aiResponse: message,
      shouldSendWhatsApp: true,
      conversationOutcome: 'timeout_warning',
      updatedContext: context,
      telemetryData: {
        intent: 'timeout_warning',
        confidence: 1.0,
        decision_method: 'timeout',
        flow_lock_active: !!context.flow_lock?.active_flow,
        processing_time_ms: 0
      }
    };
  }

  private handleBlockedIntent(context: EnhancedConversationContext, intentResult: any): WebhookOrchestrationResult {
    const currentFlow = context.flow_lock?.active_flow;
    const message = `Vamos terminar ${currentFlow} primeiro. Como posso continuar ajudando?`;
    
    return {
      aiResponse: message,
      shouldSendWhatsApp: true,
      conversationOutcome: 'intent_blocked_by_flow_lock',
      updatedContext: context,
      telemetryData: {
        intent: intentResult.intent,
        confidence: intentResult.confidence,
        decision_method: intentResult.decision_method,
        flow_lock_active: true,
        processing_time_ms: 0
      }
    };
  }

  /**
   * Geradores de resposta
   */
  
  private generatePricingResponse(tenantConfig: any): string {
    const services = tenantConfig?.services || [];
    if (services.length === 0) {
      return 'Entre em contato para informa√ß√µes sobre pre√ßos.';
    }

    let response = 'üí∞ Nossos pre√ßos:\n\n';
    services.slice(0, 5).forEach((service: any) => {
      response += `‚Ä¢ ${service.name}: R$ ${service.price}\n`;
    });
    
    return response;
  }

  private generateInstitutionalResponse(intent: string, tenantConfig: any): string {
    const policies = tenantConfig?.policies || {};
    
    const responses: Record<string, string> = {
      'institutional_address': policies.address || 'Consulte nosso site para endere√ßo.',
      'institutional_hours': policies.hours || 'Segunda a sexta, 8h √†s 18h.',
      'institutional_policy': policies.cancellation || 'Cancelamentos com 24h de anteced√™ncia.',
      'institutional_payment': 'Aceitamos dinheiro, cart√£o e PIX.',
      'institutional_contact': tenantConfig?.phone || 'Entre em contato pelo WhatsApp.'
    };

    return responses[intent] || 'Informa√ß√£o n√£o dispon√≠vel no momento.';
  }

  /**
   * üö® M√âTODO CR√çTICO: Gera resposta via OpenAI com contexto do Flow Lock
   * Flow Lock apenas gerencia estado, OpenAI gera TODAS as respostas
   */
  private async generateAIResponseWithFlowContext(
    messageText: string,
    intentResult: any,
    flowDecision: any,
    context: EnhancedConversationContext,
    tenantConfig: any
  ): Promise<{ response: string; outcome: string; newFlowLock?: any; llmMetrics?: any }> {
    
    const intent = intentResult.intent;
    const currentFlow: string | null = context.flow_lock?.active_flow || null;
    const currentStep: string | null = context.flow_lock?.step || null;

    // === COMANDOS DIRETOS COM FLOW LOCK (sem OpenAI) ===
    
    if (intent === 'flow_cancel') {
      const abandonedLock = this.flowManager.abandonFlow(context, 'user_requested');
      return {
        response: 'Cancelado. Como posso ajudar?',
        outcome: 'flow_cancelled',
        newFlowLock: abandonedLock
      };
    }

    if (intent === 'booking_confirm' && currentFlow === 'booking') {
      const completedLock = this.flowManager.completeFlow(context, 'appointment_booked');
      return {
        response: '‚úÖ Agendamento confirmado! Voc√™ receber√° um lembrete por email.',
        outcome: 'appointment_booked',
        newFlowLock: completedLock
      };
    }

    if (intent === 'slot_selection' && currentFlow === 'booking') {
      const nextLock = this.flowManager.advanceStep(context, 'confirm', { selectedSlot: messageText });
      return {
        response: `Confirma agendamento no hor√°rio ${messageText}? Digite "confirmo" para finalizar.`,
        outcome: 'booking_slot_selected',
        newFlowLock: nextLock
      };
    }

    // === USAR OPENAI PARA TODAS AS OUTRAS RESPOSTAS ===
    console.log('üö® M√âTODO CHAMADO: generateAIResponseWithFlowContext');
    console.log('üîç DEBUG - Entrando no bloco OpenAI:', { intent, currentFlow, currentStep });
    const startTime = Date.now();
    
    try {
      // Construir contexto para OpenAI
      const businessInfo = this.buildBusinessContext(tenantConfig);
      const flowContext = this.buildFlowContext(currentFlow, currentStep, intent);
      
      const systemPrompt = `Voc√™ √© a assistente oficial do ${tenantConfig.name || 'neg√≥cio'}. Seu papel √© atender com clareza, honestidade e objetividade, sempre em tom natural.

‚ö†Ô∏è REGRAS DE HONESTIDADE ABSOLUTA - OBRIGAT√ìRIAS:
- NUNCA invente dados. NUNCA prometa retorno. NUNCA mencione atendente humano.
- Para informa√ß√µes inexistentes (endere√ßo, hor√°rios, pagamento, pol√≠ticas) use SEMPRE a frase exata: "Infelizmente neste momento n√£o possuo esta informa√ß√£o no sistema."
- Use APENAS dados reais do sistema. Zero inven√ß√µes. Zero promessas.
- PROIBIDO inventar: hor√°rios, endere√ßos, formas de pagamento, telefones, pol√≠ticas.

${businessInfo}

üéØ DADOS PERMITIDOS (somente se existirem):
- Servi√ßos com pre√ßos reais
- Agendamentos confirmados  
- Profissionais cadastrados

üö´ DADOS PROIBIDOS (sempre usar frase padr√£o):
- Hor√°rios de funcionamento
- Endere√ßo/localiza√ß√£o  
- Formas de pagamento
- Contatos telef√¥nicos
- Pol√≠ticas n√£o confirmadas

IMPORTANTE: Responda APENAS com a mensagem honesta para o cliente. Se n√£o souber, use exatamente: "Infelizmente neste momento n√£o possuo esta informa√ß√£o no sistema."`;

      const userPrompt = `Mensagem do cliente: "${messageText}"
Inten√ß√£o detectada: ${intent}`;

      console.log('ü§ñ Chamando OpenAI para gerar resposta...');
      
      const completion = await this.openai.chat.completions.create({
        model: process.env.OPENAI_MODEL || 'gpt-4',
        temperature: 0.7,
        max_tokens: 300,
        logprobs: true,
        top_logprobs: 3,
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: userPrompt }
        ]
      });

      const aiResponse = completion.choices[0]?.message?.content?.trim() || 'üö® FALLBACK: generateAIResponseWithFlowContext - N√£o entendi. Pode reformular?';
      const latencyMs = Date.now() - startTime;

      // Calcular confidence score baseado na resposta da IA
      const calculateAIConfidenceScore = (completion: any): number => {
        try {
          // Se n√£o h√° logprobs, usar confian√ßa baseada na finish_reason
          console.log('üîç DEBUG LOGPROBS:', completion.choices[0]?.logprobs);
          if (!completion.choices[0]?.logprobs?.content) {
            const finishReason = completion.choices[0]?.finish_reason;
            console.log('üîç DEBUG NO LOGPROBS, using finish_reason:', finishReason);
            switch (finishReason) {
              case 'stop': return 0.85; // Resposta completa
              case 'length': return 0.60; // Truncada por limite
              case 'content_filter': return 0.30; // Filtrada
              default: return 0.50; // Raz√£o desconhecida
            }
          }

          // Calcular confidence m√©dio dos tokens usando logprobs
          const logprobs = completion.choices[0].logprobs.content;
          if (!logprobs || logprobs.length === 0) return 0.70;

          const avgLogprob = logprobs.reduce((sum: number, token: any) => {
            return sum + (token.logprob || -2.0);
          }, 0) / logprobs.length;

          // Converter logprob para confidence (0-1)
          // logprobs s√£o valores negativos, quanto maior (menos negativo) melhor
          // -0.5 = alta confian√ßa (~0.9), -3.0 = baixa confian√ßa (~0.3)
          const confidence = Math.max(0.1, Math.min(0.99, Math.exp(avgLogprob / -2.0)));
          
          return Math.round(confidence * 100) / 100; // 2 decimais
        } catch (error) {
          console.warn('Erro calculando AI confidence:', error);
          return 0.70; // Fallback seguro
        }
      };

      const aiConfidenceScore = calculateAIConfidenceScore(completion);
      console.log('üîç DEBUG AI CONFIDENCE CALCULATED:', aiConfidenceScore);

      // ‚úÖ CAPTURAR M√âTRICAS LLM COM VALIDA√á√ÉO
      const usage = completion.usage;
      const apiCost = this.calculateOpenAICost(usage);
      
      // ‚úÖ VALIDA√á√ÉO: Verificar se usage est√° dispon√≠vel
      if (!usage || !usage.total_tokens) {
        console.warn('‚ö†Ô∏è OpenAI usage data n√£o dispon√≠vel', { usage, completion });
      }
      
      // ‚úÖ CALCULAR PROCESSING COST de forma mais realista
      const calculateProcessingCost = (apiCost: number | null): number => {
        if (!apiCost) return 0.00001; // Custo m√≠nimo para opera√ß√µes sem API
        
        // ‚úÖ L√ìGICA REALISTA: 10% do custo da API + custo fixo de infraestrutura
        const percentageCost = apiCost * 0.10; // 10% do custo da API
        const infrastructureCost = 0.00002; // Custo fixo de infraestrutura (Supabase, Redis, etc.)
        const databaseCost = 0.00001; // Custo de 2 INSERTs na conversation_history
        
        return Math.round((percentageCost + infrastructureCost + databaseCost) * 100000) / 100000;
      };
      
      const llmMetrics = {
        prompt_tokens: usage?.prompt_tokens ?? null,
        completion_tokens: usage?.completion_tokens ?? null,
        total_tokens: usage?.total_tokens ?? null,
        api_cost_usd: apiCost,
        processing_cost_usd: (() => {
          if (!apiCost) return 0.00001; // custo m√≠nimo
          const percentageCost = apiCost * 0.10; // 10% do custo da API
          const infrastructureCost = 0.00002; // custo fixo de infraestrutura
          const databaseCost = 0.00001; // custo de inserts no conversation_history
          return Math.round((apiCost + percentageCost + infrastructureCost + databaseCost) * 100000) / 100000;
        })(),
        confidence_score: aiConfidenceScore,
        latency_ms: latencyMs
      };

      console.log('‚úÖ OpenAI respondeu:', {
        intent,
        tokens: usage?.total_tokens,
        latency: latencyMs,
        cost: llmMetrics.api_cost_usd
      });

      // Determinar novo estado do Flow Lock baseado na inten√ß√£o
      const newFlowLock = this.determineNewFlowState(intent, currentFlow, context);
      
      // üö® CORRE√á√ÉO CR√çTICA: Outcome ser√° determinado pelo shouldPersistOutcome, n√£o aqui
      // generateAIResponseWithFlowContext n√£o deve determinar outcomes
      const outcome = null as any; // AI responses n√£o finalizam conversas por si s√≥

      return {
        response: aiResponse,
        outcome,
        newFlowLock,
        llmMetrics
      };

    } catch (error: any) {
      console.error('‚ùå Erro ao chamar OpenAI:', error);
      console.error('‚ùå Stack trace:', error?.stack);
      console.error('‚ùå Error details:', { message: error?.message, code: error?.code, status: error?.status });
      
      // Fallback para resposta determin√≠stica
      return await this.executeFlowAction(
        messageText,
        intentResult,
        flowDecision,
        context,
        tenantConfig
      );
    }
  }

  /**
   * Constr√≥i contexto do neg√≥cio para OpenAI
   */
  private buildBusinessContext(tenantConfig: any): string {
    const services = tenantConfig?.services?.slice(0, 5) || [];
    const policies = tenantConfig?.policies || {};
    
    let context = `SOBRE O NEG√ìCIO:
- Nome: ${tenantConfig.name || 'N√£o informado'}
- Tipo: ${tenantConfig.domain || 'Servi√ßos gerais'}`;

    if (services.length > 0) {
      context += `\n- Servi√ßos: ${services.map((s: any) => `${s.name} (R$ ${s.price})`).join(', ')}`;
    }

    if (policies.address) {
      context += `\n- Endere√ßo: ${policies.address}`;
    }

    if (policies.hours) {
      context += `\n- Hor√°rio: ${policies.hours}`;
    }

    return context;
  }

  /**
   * Constr√≥i contexto do fluxo atual para OpenAI
   */
  private buildFlowContext(currentFlow: string | null, currentStep: string | null, intent: string): string {
    if (!currentFlow) {
      return `O cliente est√° iniciando uma nova conversa. Inten√ß√£o detectada: ${intent}`;
    }

    return `O cliente est√° no meio de um fluxo de ${currentFlow}, etapa: ${currentStep || 'inicial'}. Inten√ß√£o detectada: ${intent}`;
  }

  /**
   * Calcula custo estimado da chamada OpenAI
   */
  private calculateOpenAICost(usage: any): number | null {
    if (!usage || !usage.prompt_tokens || !usage.completion_tokens) {
      return null;
    }

    const promptCost = (usage.prompt_tokens / 1000) * (parseFloat(process.env.OPENAI_PROMPT_COST_PER_1K || '0.03'));
    const completionCost = (usage.completion_tokens / 1000) * (parseFloat(process.env.OPENAI_COMPLETION_COST_PER_1K || '0.06'));
    
    return Math.round((promptCost + completionCost) * 100000) / 100000; // 5 casas decimais
  }

  /**
   * Determina novo estado do Flow Lock baseado na inten√ß√£o
   */
  private determineNewFlowState(intent: string | null, currentFlow: string | null, context: EnhancedConversationContext): any {
    const targetFlow = this.mapIntentToFlow(intent);
    
    if (!targetFlow) {
      return context.flow_lock; // Manter estado atual
    }

    // Iniciar novo fluxo se n√£o h√° fluxo ativo
    if (!currentFlow) {
      return this.flowManager.startFlowLock(targetFlow, 'start');
    }

    // Continuar fluxo atual
    return context.flow_lock;
  }

  /**
   * Detecta se a conversa est√° finalizada e deve persistir outcome
   * Retorna null se conversa ainda est√° em andamento
   */
  private shouldPersistOutcome(intent: string | null, response: string, context: EnhancedConversationContext): string | null {
    // üîß MODO VALIDA√á√ÉO: Persistir todas as mensagens para an√°lise de intents
    if (process.env.ENABLE_INTENT_VALIDATION === 'true') {
      return this.determineConversationOutcome(intent, response, true);
    }
    
    // üö® CORRE√á√ÉO CR√çTICA: Outcome deve ser NULL para conversas em andamento
    // S√≥ persistir quando conversa REALMENTE finaliza
    
    // 1. INTENTS FINALIZADORES EXPL√çCITOS - apenas confirma√ß√µes que criam/modificam appointments
    const trulyFinalizingIntents = [
      'booking_confirm', 'cancel_confirm', 'reschedule_confirm'
    ];
    
    if (intent && trulyFinalizingIntents.includes(intent)) {
      return this.determineConversationOutcome(intent, response);
    }
    
    // 2. TIMEOUT DE INATIVIDADE - s√≥ ser√° processado pelo cronjob, n√£o aqui
    // Removida l√≥gica de timeout pois ser√° handled pelo ConversationOutcomeProcessor
    
    // 3. TODAS AS OUTRAS INTERA√á√ïES - conversa ainda em andamento
    // Includes: greeting, pricing, booking, address, business_hours, etc.
    // Essas s√£o intera√ß√µes dentro da conversa, n√£o o fim dela
    return null;
  }

  /**
   * Determina outcome da conversa baseado na inten√ß√£o e resposta
   * APENAS para intents que realmente finalizam conversa (ou modo valida√ß√£o)
   */
  private determineConversationOutcome(intent: string | null, response: string, isValidationMode: boolean = false): string {
    // üîß MODO VALIDA√á√ÉO: Persistir todas as mensagens para an√°lise
    if (isValidationMode) {
      // Em modo valida√ß√£o, usar o intent como outcome para an√°lise
      return `validation_${intent || 'null'}`;
    }
    
    // Mapear APENAS intents finalizadores para outcomes v√°lidos
    const finalizingOutcomeMap: Record<string, string> = {
      'booking_confirm': 'appointment_created',
      'cancel_confirm': 'appointment_cancelled', 
      'reschedule_confirm': 'appointment_modified'
    };

    // Se n√£o √© um intent finalizador, isso √© um erro de l√≥gica
    if (!intent || !finalizingOutcomeMap[intent]) {
      console.error(`üö® ERRO: determineConversationOutcome chamado para intent n√£o-finalizador: ${intent}`);
      return 'error';
    }

    return finalizingOutcomeMap[intent];
  }

  /**
   * DETECTAR E PERSISTIR OUTCOME QUANDO CONVERSA FINALIZA
   * Chama ConversationOutcomeAnalyzerService para an√°lise contextual
   */
  async checkAndPersistConversationOutcome(
    sessionId: string,
    trigger: 'timeout' | 'flow_completion' | 'appointment_action' | 'user_exit' = 'flow_completion'
  ): Promise<void> {
    try {
      // Usar o novo servi√ßo de an√°lise contextual
      const analysis = await this.outcomeAnalyzer.analyzeConversationOutcome(sessionId, trigger);
      
      if (analysis) {
        // Persistir outcome APENAS na √∫ltima mensagem AI
        const success = await this.outcomeAnalyzer.persistOutcomeToFinalMessage(analysis);
        
        if (success) {
          console.log(`üéØ Conversation outcome persisted: ${analysis.outcome} (${analysis.confidence})`);
        }
      }
    } catch (error) {
      console.error('‚ùå Failed to check conversation outcome:', error);
    }
  }

  /**
   * EXECUTAR AN√ÅLISE PERI√ìDICA DE CONVERSAS FINALIZADAS
   * Deve ser chamado por cronjob para processar conversas abandonadas por timeout
   */
  async processFinishedConversations(): Promise<void> {
    try {
      await this.outcomeAnalyzer.checkForFinishedConversations();
    } catch (error) {
      console.error('‚ùå Failed to process finished conversations:', error);
    }
  }

  /**
   * üö® FUN√á√ÉO REMOVIDA: tryDeterministicResponse
   * Todas as respostas devem usar OpenAI para capturar m√©tricas LLM corretas
   */

  /**
   * Classifica√ß√£o LLM determin√≠stica e fechada
   * Usa temperature=0 e top_p=0 para m√°xima consist√™ncia
   */
  private async classifyIntentWithLLM(text: string): Promise<string | null> {
    const SYSTEM_PROMPT = `Voc√™ √© um classificador de inten√ß√£o. Classifique a mensagem do usu√°rio em EXATAMENTE UMA das chaves abaixo e nada al√©m disso.

INTENTS PERMITIDAS:
- greeting
- services
- pricing
- availability
- my_appointments
- address
- payments
- business_hours
- cancel
- reschedule
- confirm
- modify_appointment
- policies
- wrong_number
- test_message
- booking_abandoned
- noshow_followup

Regras:
1) Responda SOMENTE com JSON no formato: {"intent":"<uma-das-chaves>"}.
2) Se N√ÉO for poss√≠vel classificar com seguran√ßa, responda exatamente: {"intent":null}.
3) N√£o explique. N√£o inclua texto extra. Sem sin√¥nimos fora da lista.`;

    try {
      const completion = await this.openai.chat.completions.create({
        model: process.env.OPENAI_MODEL || 'gpt-4',
        temperature: 0,
        top_p: 0,
        max_tokens: 20,
        messages: [
          { role: 'system', content: SYSTEM_PROMPT },
          { role: 'user', content: `Mensagem do usu√°rio (pt-BR):\n---\n${text}\n---\nClassifique.` }
        ]
      });

      const raw = completion.choices?.[0]?.message?.content?.trim() || '';
      let parsed: { intent: string | null } | null = null;
      try { parsed = JSON.parse(raw); } catch { return null; }

      if (!parsed || typeof parsed.intent === 'undefined') return null;
      if (parsed.intent === null) return null;

      // Validar contra a allowlist FINAL
      return INTENT_KEYS.includes(parsed.intent as any) ? parsed.intent : null;

    } catch (error) {
      console.error('‚ùå LLM intent classification failed:', error);
      return null;
    }
  }
}