/**
 * Campaign Document Processor Service
 *
 * Processa documentos de campanha para RAG:
 * 1. Recebe PDF/DOCX/TXT
 * 2. Extrai texto
 * 3. Divide em chunks inteligentes
 * 4. Gera embeddings via OpenAI
 * 5. Persiste em campaign_documents
 */

import OpenAI from "openai";
import { createClient, SupabaseClient } from "@supabase/supabase-js";
import * as fs from "fs";
import * as path from "path";

// =====================================================
// TIPOS E INTERFACES
// =====================================================

export interface DocumentUpload {
  campaignId: string | null; // null = documento global
  title: string;
  docType: "briefing" | "landing_page" | "knowledge" | "faq" | "product" | "policy" | "script" | "other";
  content?: string; // Texto direto
  filePath?: string; // Caminho do arquivo
  fileBuffer?: Buffer; // Buffer do arquivo
  fileName?: string; // Nome do arquivo
  sourceUrl?: string;
  metadata?: Record<string, any>;
}

export interface ChunkOptions {
  maxTokens: number; // Tamanho m√°ximo do chunk (padr√£o: 500)
  overlapTokens: number; // Sobreposi√ß√£o entre chunks (padr√£o: 100)
  minTokens: number; // Tamanho m√≠nimo para criar chunk (padr√£o: 50)
}

export interface ProcessingResult {
  success: boolean;
  documentId?: string;
  chunksCreated: number;
  totalTokens: number;
  error?: string;
}

interface DocumentChunk {
  content: string;
  chunkNumber: number;
  tokenCount: number;
}

// =====================================================
// CONSTANTES
// =====================================================

const DEFAULT_CHUNK_OPTIONS: ChunkOptions = {
  maxTokens: 500,
  overlapTokens: 100,
  minTokens: 50,
};

const EMBEDDING_MODEL = "text-embedding-3-small";
const EMBEDDING_DIMENSION = 1536;

// Aproxima√ß√£o: 1 token ~= 4 caracteres em portugu√™s
const CHARS_PER_TOKEN = 4;

// =====================================================
// SERVI√áO PRINCIPAL
// =====================================================

export class CampaignDocumentProcessorService {
  private openai: OpenAI;
  private supabase: SupabaseClient;

  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY || "",
    });

    this.supabase = createClient(
      process.env.SUPABASE_URL || "",
      process.env.SUPABASE_SERVICE_ROLE_KEY || "",
    );
  }

  // =====================================================
  // M√âTODO PRINCIPAL: Processar documento completo
  // =====================================================

  async processDocument(
    upload: DocumentUpload,
    options: Partial<ChunkOptions> = {},
  ): Promise<ProcessingResult> {
    const chunkOptions = { ...DEFAULT_CHUNK_OPTIONS, ...options };

    try {
      console.log(
        `üìÑ Processando documento: ${upload.title} (campaign: ${upload.campaignId || "GLOBAL"})`,
      );

      // 1. Extrair texto do documento
      let text = "";
      if (upload.content) {
        text = upload.content;
      } else if (upload.fileBuffer && upload.fileName) {
        text = await this.extractText(upload.fileBuffer, upload.fileName);
      } else if (upload.filePath) {
        const buffer = fs.readFileSync(upload.filePath);
        const fileName = path.basename(upload.filePath);
        text = await this.extractText(buffer, fileName);
      } else {
        throw new Error("Nenhum conte√∫do fornecido para processamento");
      }

      if (!text || text.trim().length === 0) {
        throw new Error("Documento vazio ou n√£o foi poss√≠vel extrair texto");
      }

      console.log(
        `üìù Texto extra√≠do: ${text.length} caracteres (~${Math.ceil(text.length / CHARS_PER_TOKEN)} tokens)`,
      );

      // 2. Limpar e normalizar texto
      text = this.cleanText(text);

      // 3. Dividir em chunks
      const chunks = this.createChunks(text, chunkOptions);
      console.log(`üì¶ Criados ${chunks.length} chunks`);

      if (chunks.length === 0) {
        throw new Error("N√£o foi poss√≠vel criar chunks do documento");
      }

      // 4. Gerar embeddings e salvar
      let chunksCreated = 0;
      let totalTokens = 0;
      let documentId: string | undefined;

      for (const chunk of chunks) {
        try {
          // Gerar embedding
          const embedding = await this.generateEmbedding(chunk.content);

          // Salvar no banco
          const { data, error } = await this.supabase
            .from("campaign_documents")
            .insert({
              campaign_id: upload.campaignId,
              doc_type: upload.docType,
              title: upload.title,
              content: chunk.content,
              content_chunk: chunk.chunkNumber,
              embedding: embedding,
              metadata: {
                ...upload.metadata,
                original_file: upload.fileName || null,
                token_count: chunk.tokenCount,
                total_chunks: chunks.length,
              },
              source_url: upload.sourceUrl,
              is_active: true,
            })
            .select("id")
            .single();

          if (error) {
            console.error(
              `‚ùå Erro ao salvar chunk ${chunk.chunkNumber}:`,
              error,
            );
            continue;
          }

          if (!documentId && data) {
            documentId = data.id;
          }

          chunksCreated++;
          totalTokens += chunk.tokenCount;

          console.log(
            `‚úÖ Chunk ${chunk.chunkNumber}/${chunks.length} salvo (${chunk.tokenCount} tokens)`,
          );

          // Rate limiting: pequena pausa entre chamadas
          await this.sleep(100);
        } catch (chunkError) {
          console.error(
            `‚ùå Erro ao processar chunk ${chunk.chunkNumber}:`,
            chunkError,
          );
        }
      }

      console.log(
        `üéâ Documento processado: ${chunksCreated}/${chunks.length} chunks, ${totalTokens} tokens totais`,
      );

      return {
        success: chunksCreated > 0,
        documentId,
        chunksCreated,
        totalTokens,
      };
    } catch (error) {
      const errorMessage =
        error instanceof Error ? error.message : String(error);
      console.error(`‚ùå Erro ao processar documento:`, errorMessage);

      return {
        success: false,
        chunksCreated: 0,
        totalTokens: 0,
        error: errorMessage,
      };
    }
  }

  // =====================================================
  // EXTRA√á√ÉO DE TEXTO
  // =====================================================

  private async extractText(buffer: Buffer, fileName: string): Promise<string> {
    const extension = path.extname(fileName).toLowerCase();

    switch (extension) {
      case ".txt":
        return buffer.toString("utf-8");

      case ".pdf":
        return await this.extractFromPdf(buffer);

      case ".docx":
        return await this.extractFromDocx(buffer);

      case ".md":
        return buffer.toString("utf-8");

      default:
        throw new Error(`Formato de arquivo n√£o suportado: ${extension}`);
    }
  }

  private async extractFromPdf(buffer: Buffer): Promise<string> {
    try {
      // Importa√ß√£o din√¢mica do pdf-parse
      const pdfParse = require("pdf-parse");
      const data = await pdfParse(buffer);
      return data.text || "";
    } catch (error) {
      console.error("Erro ao extrair texto do PDF:", error);
      throw new Error(
        "Falha ao extrair texto do PDF. Verifique se pdf-parse est√° instalado.",
      );
    }
  }

  private async extractFromDocx(buffer: Buffer): Promise<string> {
    try {
      // Importa√ß√£o din√¢mica do mammoth
      const mammoth = require("mammoth");
      const result = await mammoth.extractRawText({ buffer });
      return result.value || "";
    } catch (error) {
      console.error("Erro ao extrair texto do DOCX:", error);
      throw new Error(
        "Falha ao extrair texto do DOCX. Verifique se mammoth est√° instalado.",
      );
    }
  }

  // =====================================================
  // CHUNKING INTELIGENTE
  // =====================================================

  private createChunks(text: string, options: ChunkOptions): DocumentChunk[] {
    const chunks: DocumentChunk[] = [];

    // Dividir por par√°grafos primeiro
    const paragraphs = text.split(/\n\n+/).filter((p) => p.trim().length > 0);

    let currentChunk = "";
    let currentTokens = 0;
    let chunkNumber = 1;

    for (const paragraph of paragraphs) {
      const paragraphTokens = this.estimateTokens(paragraph);

      // Se o par√°grafo sozinho excede o m√°ximo, dividir por senten√ßas
      if (paragraphTokens > options.maxTokens) {
        // Salvar chunk atual se existir
        if (currentChunk.trim()) {
          chunks.push({
            content: currentChunk.trim(),
            chunkNumber: chunkNumber++,
            tokenCount: currentTokens,
          });
          currentChunk = "";
          currentTokens = 0;
        }

        // Dividir par√°grafo grande em chunks menores
        const subChunks = this.splitLargeParagraph(paragraph, options);
        for (const subChunk of subChunks) {
          chunks.push({
            content: subChunk.content,
            chunkNumber: chunkNumber++,
            tokenCount: subChunk.tokenCount,
          });
        }
        continue;
      }

      // Verificar se adicionar este par√°grafo excede o limite
      if (currentTokens + paragraphTokens > options.maxTokens) {
        // Salvar chunk atual
        if (currentChunk.trim()) {
          chunks.push({
            content: currentChunk.trim(),
            chunkNumber: chunkNumber++,
            tokenCount: currentTokens,
          });
        }

        // Iniciar novo chunk com overlap
        const overlap = this.getOverlapText(currentChunk, options.overlapTokens);
        currentChunk = overlap + (overlap ? "\n\n" : "") + paragraph;
        currentTokens =
          this.estimateTokens(overlap) + paragraphTokens;
      } else {
        // Adicionar ao chunk atual
        currentChunk += (currentChunk ? "\n\n" : "") + paragraph;
        currentTokens += paragraphTokens;
      }
    }

    // Salvar √∫ltimo chunk se tiver tamanho m√≠nimo
    if (currentChunk.trim() && currentTokens >= options.minTokens) {
      chunks.push({
        content: currentChunk.trim(),
        chunkNumber: chunkNumber,
        tokenCount: currentTokens,
      });
    }

    return chunks;
  }

  private splitLargeParagraph(
    paragraph: string,
    options: ChunkOptions,
  ): DocumentChunk[] {
    const chunks: DocumentChunk[] = [];
    const sentences = paragraph.split(/(?<=[.!?])\s+/);

    let currentChunk = "";
    let currentTokens = 0;
    let localChunkNum = 1;

    for (const sentence of sentences) {
      const sentenceTokens = this.estimateTokens(sentence);

      if (currentTokens + sentenceTokens > options.maxTokens) {
        if (currentChunk.trim()) {
          chunks.push({
            content: currentChunk.trim(),
            chunkNumber: localChunkNum++,
            tokenCount: currentTokens,
          });
        }

        // Se a senten√ßa sozinha √© muito grande, dividir por palavras
        if (sentenceTokens > options.maxTokens) {
          const wordChunks = this.splitByWords(sentence, options.maxTokens);
          for (const wc of wordChunks) {
            chunks.push({
              content: wc,
              chunkNumber: localChunkNum++,
              tokenCount: this.estimateTokens(wc),
            });
          }
          currentChunk = "";
          currentTokens = 0;
        } else {
          currentChunk = sentence;
          currentTokens = sentenceTokens;
        }
      } else {
        currentChunk += (currentChunk ? " " : "") + sentence;
        currentTokens += sentenceTokens;
      }
    }

    if (currentChunk.trim()) {
      chunks.push({
        content: currentChunk.trim(),
        chunkNumber: localChunkNum,
        tokenCount: currentTokens,
      });
    }

    return chunks;
  }

  private splitByWords(text: string, maxTokens: number): string[] {
    const words = text.split(/\s+/);
    const chunks: string[] = [];
    let currentChunk = "";
    let currentTokens = 0;

    for (const word of words) {
      const wordTokens = this.estimateTokens(word);

      if (currentTokens + wordTokens > maxTokens) {
        if (currentChunk.trim()) {
          chunks.push(currentChunk.trim());
        }
        currentChunk = word;
        currentTokens = wordTokens;
      } else {
        currentChunk += (currentChunk ? " " : "") + word;
        currentTokens += wordTokens;
      }
    }

    if (currentChunk.trim()) {
      chunks.push(currentChunk.trim());
    }

    return chunks;
  }

  private getOverlapText(text: string, overlapTokens: number): string {
    if (!text || overlapTokens <= 0) return "";

    const words = text.split(/\s+/);
    const targetChars = overlapTokens * CHARS_PER_TOKEN;

    let overlap = "";
    for (let i = words.length - 1; i >= 0 && overlap.length < targetChars; i--) {
      overlap = words[i] + (overlap ? " " + overlap : "");
    }

    return overlap;
  }

  private estimateTokens(text: string): number {
    return Math.ceil(text.length / CHARS_PER_TOKEN);
  }

  // =====================================================
  // EMBEDDING
  // =====================================================

  private async generateEmbedding(text: string): Promise<number[]> {
    try {
      const response = await this.openai.embeddings.create({
        model: EMBEDDING_MODEL,
        input: text,
      });

      const embedding = response.data[0]?.embedding;
      if (!embedding || embedding.length !== EMBEDDING_DIMENSION) {
        throw new Error("Embedding inv√°lido retornado pela OpenAI");
      }

      return embedding;
    } catch (error) {
      console.error("Erro ao gerar embedding:", error);
      throw error;
    }
  }

  // =====================================================
  // UTILIT√ÅRIOS
  // =====================================================

  private cleanText(text: string): string {
    return (
      text
        // Remover m√∫ltiplas quebras de linha
        .replace(/\n{3,}/g, "\n\n")
        // Remover espa√ßos m√∫ltiplos
        .replace(/[ \t]+/g, " ")
        // Remover caracteres de controle
        .replace(/[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]/g, "")
        // Normalizar aspas
        .replace(/[""]/g, '"')
        .replace(/['']/g, "'")
        // Trim
        .trim()
    );
  }

  private sleep(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }

  // =====================================================
  // M√âTODOS DE GEST√ÉO DE DOCUMENTOS
  // =====================================================

  /**
   * Lista documentos de uma campanha
   */
  async listDocuments(
    campaignId: string | null,
  ): Promise<{ id: string; title: string; doc_type: string; chunks: number }[]> {
    const query = this.supabase
      .from("campaign_documents")
      .select("id, title, doc_type, content_chunk")
      .eq("is_active", true);

    if (campaignId) {
      query.eq("campaign_id", campaignId);
    } else {
      query.is("campaign_id", null);
    }

    const { data, error } = await query.order("title");

    if (error) {
      console.error("Erro ao listar documentos:", error);
      return [];
    }

    // Agrupar por t√≠tulo para contar chunks
    const grouped = (data || []).reduce(
      (acc, doc) => {
        const key = doc.title;
        if (!acc[key]) {
          acc[key] = {
            id: doc.id,
            title: doc.title,
            doc_type: doc.doc_type,
            chunks: 0,
          };
        }
        acc[key].chunks++;
        return acc;
      },
      {} as Record<string, any>,
    );

    return Object.values(grouped);
  }

  /**
   * Desativa um documento (soft delete)
   */
  async deactivateDocument(
    title: string,
    campaignId: string | null,
  ): Promise<boolean> {
    const query = this.supabase
      .from("campaign_documents")
      .update({ is_active: false, updated_at: new Date().toISOString() })
      .eq("title", title);

    if (campaignId) {
      query.eq("campaign_id", campaignId);
    } else {
      query.is("campaign_id", null);
    }

    const { error } = await query;

    if (error) {
      console.error("Erro ao desativar documento:", error);
      return false;
    }

    return true;
  }

  /**
   * Reprocessa um documento existente
   */
  async reprocessDocument(
    title: string,
    campaignId: string | null,
    options: Partial<ChunkOptions> = {},
  ): Promise<ProcessingResult> {
    // Buscar conte√∫do original (primeiro chunk)
    const query = this.supabase
      .from("campaign_documents")
      .select("*")
      .eq("title", title)
      .eq("content_chunk", 1);

    if (campaignId) {
      query.eq("campaign_id", campaignId);
    } else {
      query.is("campaign_id", null);
    }

    const { data, error } = await query.single();

    if (error || !data) {
      return {
        success: false,
        chunksCreated: 0,
        totalTokens: 0,
        error: "Documento n√£o encontrado",
      };
    }

    // Desativar vers√£o antiga
    await this.deactivateDocument(title, campaignId);

    // Reprocessar
    return this.processDocument(
      {
        campaignId,
        title: data.title,
        docType: data.doc_type,
        content: data.content,
        sourceUrl: data.source_url,
        metadata: data.metadata,
      },
      options,
    );
  }

  // =====================================================
  // EXTRA√á√ÉO DE CAMPOS DA CAMPANHA VIA GPT
  // =====================================================

  /**
   * Extrai Nicho, P√∫blico Alvo e Descri√ß√£o dos documentos da campanha
   */
  async extractCampaignFields(campaignId: string): Promise<{
    success: boolean;
    nicho?: string;
    publicoAlvo?: string;
    descricaoServico?: string;
    error?: string;
  }> {
    try {
      console.log(`üìÑ Extraindo campos da campanha: ${campaignId}`);

      // 1. Buscar todos os chunks de documentos da campanha
      const { data: documents, error } = await this.supabase
        .from("campaign_documents")
        .select("title, content, doc_type, content_chunk")
        .eq("campaign_id", campaignId)
        .eq("is_active", true)
        .order("title")
        .order("content_chunk");

      if (error) {
        console.error("Erro ao buscar documentos:", error);
        return { success: false, error: "Erro ao buscar documentos da campanha" };
      }

      if (!documents || documents.length === 0) {
        return { success: false, error: "Nenhum documento encontrado para esta campanha" };
      }

      console.log(`üìö Encontrados ${documents.length} chunks de documentos`);

      // 2. Concatenar conte√∫do dos documentos (limitar a ~8000 tokens para GPT)
      let combinedContent = "";
      const maxChars = 32000; // ~8000 tokens

      for (const doc of documents) {
        if (combinedContent.length + doc.content.length > maxChars) {
          break;
        }
        combinedContent += `\n\n--- ${doc.title} (${doc.doc_type}) ---\n${doc.content}`;
      }

      console.log(`üìù Conte√∫do combinado: ${combinedContent.length} caracteres`);

      // 3. Usar GPT para extrair os campos
      const response = await this.openai.chat.completions.create({
        model: "gpt-4o-mini",
        temperature: 0.3,
        messages: [
          {
            role: "system",
            content: `Voc√™ √© um especialista em an√°lise de documentos de campanhas de marketing.
Sua tarefa √© extrair informa√ß√µes espec√≠ficas dos documentos fornecidos.

IMPORTANTE:
- Extraia informa√ß√µes REAIS dos documentos, n√£o invente
- Se uma informa√ß√£o n√£o estiver clara, use o contexto para inferir
- Mantenha as respostas concisas e diretas
- Se n√£o encontrar informa√ß√£o suficiente, indique "N√£o identificado"`
          },
          {
            role: "user",
            content: `Analise os seguintes documentos de campanha e extraia:

1. **NICHO ALVO**: Qual √© o segmento/nicho de mercado da campanha? (ex: "Advogados", "Sal√µes de Beleza", "Cl√≠nicas M√©dicas")

2. **P√öBLICO ALVO**: Quem s√£o os clientes ideais? Descreva caracter√≠sticas como profiss√£o, localiza√ß√£o, necessidades, comportamento.

3. **DESCRI√á√ÉO DO SERVI√áO/PRODUTO**: O que est√° sendo oferecido? Descreva brevemente o servi√ßo ou produto principal da campanha.

DOCUMENTOS:
${combinedContent}

Responda EXATAMENTE neste formato JSON:
{
  "nicho": "string com o nicho identificado",
  "publicoAlvo": "string descrevendo o p√∫blico-alvo",
  "descricaoServico": "string descrevendo o servi√ßo/produto"
}`
          }
        ],
        response_format: { type: "json_object" }
      });

      const content = response.choices[0]?.message?.content;
      if (!content) {
        return { success: false, error: "GPT n√£o retornou resposta" };
      }

      const extracted = JSON.parse(content);
      console.log(`‚úÖ Campos extra√≠dos:`, extracted);

      return {
        success: true,
        nicho: extracted.nicho || "N√£o identificado",
        publicoAlvo: extracted.publicoAlvo || "N√£o identificado",
        descricaoServico: extracted.descricaoServico || "N√£o identificado"
      };
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      console.error(`‚ùå Erro ao extrair campos:`, errorMessage);
      return { success: false, error: errorMessage };
    }
  }

  /**
   * Busca conte√∫do bruto dos documentos de uma campanha
   */
  async getDocumentsContent(campaignId: string): Promise<string> {
    const { data: documents, error } = await this.supabase
      .from("campaign_documents")
      .select("title, content, doc_type")
      .eq("campaign_id", campaignId)
      .eq("is_active", true)
      .order("title")
      .order("content_chunk");

    if (error || !documents || documents.length === 0) {
      return "";
    }

    return documents.map(d => `${d.title}: ${d.content}`).join("\n\n");
  }
}

// Export singleton
export const campaignDocumentProcessor = new CampaignDocumentProcessorService();
