// src/core/finalize.ts
import { ConversationRow, ConversationRowT } from "../contracts/conversation";
import { persistConversationMessage } from "../services/persistence/conversation-history.persistence";
import { recordLLMMetrics } from "../services/telemetry/llm-telemetry.service";

type FinalizeInput = {
  tenantId: string;
  userId?: string;
  sessionId?: string;
  requestText: string;
  replyText: string;
  isFromUser: boolean; // normalmente false aqui
  intentDetected: string | null; // null quando Flow Lock decide
  confidence?: number | null;
  modelUsed?: string | null;
  tokensUsed?: number | null;
  apiCostUsd?: number | null;
  processingCostUsd?: number | null; // ADICIONADO: Custo de processamento
  outcome?: string | null;
  context?: Record<string, any>;
  messageSource?: string; // 'whatsapp' ou 'whatsapp_demo'
};

export async function finalizeAndRespond(input: FinalizeInput) {
  // DEBUG: Log para rastrear messageSource
  console.log('üîç [FINALIZE-DEBUG] Input messageSource:', {
    messageSource: input.messageSource,
    tenantId: input.tenantId,
    userId: input.userId,
    intentDetected: input.intentDetected,
    modelUsed: input.modelUsed
  });

  // 1) Valida√ß√£o dos campos que vamos persistir
  const row: ConversationRowT = ConversationRow.parse({
    tenant_id: input.tenantId,
    user_id: input.userId,
    content: input.replyText,
    is_from_user: false,
    message_type: "text",
    intent_detected: input.intentDetected ?? null,
    confidence_score: input.confidence ?? null,
    conversation_context: input.context ?? {},
    model_used: input.modelUsed ?? null,
    tokens_used: input.tokensUsed ?? null,
    message_source: input.messageSource ?? 'whatsapp', // Default para whatsapp
    api_cost_usd: input.apiCostUsd ?? null,
    processing_cost_usd: input.processingCostUsd ?? null, // ADICIONADO: Custo de processamento
    conversation_outcome: null, // SEMPRE null - preenchido pelo cronjob a cada 10min
    // session_id_uuid: generated by database from conversation_context.session_id
  });

  // 2) Persist√™ncia √∫nica e central - capturar message_id
  const messageId = await persistConversationMessage(row);

  // 3) Telemetria (APENAS se houve LLM real - n√£o para deterministic/flowlock)
  const isRealLLM = input.modelUsed && 
    !['deterministic', 'flowlock', 'system'].includes(input.modelUsed) &&
    (input.tokensUsed || input.apiCostUsd);

  console.log('üîç [FINALIZE] Telemetry check:', {
    modelUsed: input.modelUsed,
    isExcluded: ['deterministic', 'flowlock', 'system'].includes(input.modelUsed || ''),
    hasTokensOrCost: !!(input.tokensUsed || input.apiCostUsd),
    isRealLLM,
    messageId
  });
    
  if (isRealLLM) {
    console.log('‚úÖ [FINALIZE] Recording LLM telemetry for:', input.modelUsed, 'with message_id:', messageId);
    await recordLLMMetrics({
      messageId, // SOLU√á√ÉO DEFINITIVA: passar o message_id real
      tenantId: input.tenantId,
      sessionId: input.sessionId,
      modelUsed: input.modelUsed ?? undefined,
      tokensUsed: input.tokensUsed ?? undefined,
      apiCostUsd: input.apiCostUsd ?? undefined,
      intent: input.intentDetected ?? undefined,
      outcome: input.outcome ?? undefined,
    });
  } else {
    console.log('üö´ [FINALIZE] Skipping telemetry for non-LLM method:', input.modelUsed);
  }

  // 4) Sempre retornar a resposta final j√° depois da persist√™ncia
  return {
    text: input.replyText,
    meta: {
      intent_detected: input.intentDetected,
      outcome: null, // outcome ser√° definido pelo cronjob
      model_used: input.modelUsed,
      tokens_used: input.tokensUsed,
      api_cost_usd: input.apiCostUsd,
      processing_cost_usd: input.processingCostUsd, // ADICIONADO: Retornar processing_cost_usd
    },
  };
}